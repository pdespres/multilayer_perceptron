{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiplyGate:\n",
    "    def forward(self,W, X):\n",
    "        return np.dot(X, W)\n",
    "\n",
    "    def backward(self, W, X, dZ):\n",
    "        dW = np.dot(np.transpose(X), dZ)\n",
    "        dX = np.dot(dZ, np.transpose(W))\n",
    "        return dW, dX\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, X, b):\n",
    "        return X + b\n",
    "\n",
    "    def backward(self, X, b, dZ):\n",
    "        dX = dZ * np.ones_like(X)\n",
    "        db = np.dot(np.ones((1, dZ.shape[0]), dtype=np.float64), dZ)\n",
    "        return db, dX\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "    def backward(self, X, top_diff):\n",
    "        output = self.forward(X)\n",
    "        return (1.0 - output) * output * top_diff\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def backward(self, X, top_diff):\n",
    "        output = self.forward(X)\n",
    "        return (1.0 - np.square(output)) * top_diff\n",
    "    \n",
    "class Softmax:\n",
    "    def predict(self, X):\n",
    "        exp_scores = np.exp(X)\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        num_examples = X.shape[0]\n",
    "        probs = self.predict(X)\n",
    "#         print(probs[:5], y[:5])\n",
    "        corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "#         print(corect_logprobs[:5])\n",
    "        data_loss = np.sum(corect_logprobs)\n",
    "#         print('data loss', 1./num_examples * data_loss)\n",
    "        return 1./num_examples * data_loss\n",
    "#         log_likelihood = -np.log(probs[range(y.shape[0]), y])\n",
    "#         print('loss', np.sum(log_likelihood) / y.shape[0])\n",
    "#         loss = np.sum(log_likelihood) / y.shape[0]\n",
    "# #         return loss\n",
    "#         return 1./num_examples * data_loss\n",
    "    \n",
    "    def diff(self, X, y):\n",
    "        num_examples = X.shape[0]\n",
    "        probs = self.predict(X)\n",
    "        print(probs[:2], y[:2])\n",
    "        probs[range(num_examples), y] -= 1\n",
    "        return probs\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, layers_dim):\n",
    "        self.b = []\n",
    "        self.W = []\n",
    "        for i in range(len(layers_dim)-1):\n",
    "            self.W.append(np.random.randn(layers_dim[i], layers_dim[i+1]) / np.sqrt(layers_dim[i]))\n",
    "            self.b.append(np.random.randn(layers_dim[i+1]).reshape(1, layers_dim[i+1]))\n",
    "#             self.W.append(np.zeros(layers_dim[i], layers_dim[i+1]))\n",
    "#             self.b.append(np.ones(layers_dim[i+1]).reshape(1, layers_dim[i+1]))\n",
    "\n",
    "    def predict(self, X):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "\n",
    "        input = X\n",
    "        for i in range(len(self.W)):\n",
    "            mul = mulGate.forward(self.W[i], input)\n",
    "            add = addGate.forward(mul, self.b[i])\n",
    "            input = layer.forward(add)\n",
    "\n",
    "        probs = softmaxOutput.predict(input)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "    \n",
    "        input = X\n",
    "        for i in range(len(self.W)):\n",
    "            mul = mulGate.forward(self.W[i], input)\n",
    "            add = addGate.forward(mul, self.b[i])\n",
    "            input = layer.forward(add)\n",
    "\n",
    "        return softmaxOutput.loss(input, y)\n",
    "\n",
    "    def train(self, X, y, num_passes=70, epsilon=0.01, reg_lambda=0.01, print_loss=False):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "\n",
    "        for epoch in range(num_passes):\n",
    "            # Forward propagation\n",
    "            input = X\n",
    "            forward = [(None, None, input)]\n",
    "            for i in range(len(self.W)):\n",
    "                mul = mulGate.forward(self.W[i], input)\n",
    "                add = addGate.forward(mul, self.b[i])\n",
    "                input = layer.forward(add)\n",
    "                forward.append((mul, add, input))\n",
    "                print('ff size z, a, mul', add.shape, input.shape, mul.shape)\n",
    "\n",
    "            # Back propagation\n",
    "            dtanh = softmaxOutput.diff(forward[len(forward)-1][2], y)\n",
    "            print('dtahn shape', dtanh.shape,dtanh[:2])\n",
    "            for i in range(len(forward)-1, 0, -1):\n",
    "                dadd = layer.backward(forward[i][1], dtanh)\n",
    "                db, dmul = addGate.backward(forward[i][0], self.b[i-1], dadd)\n",
    "                dW, dtanh = mulGate.backward(self.W[i-1], forward[i-1][2], dmul)\n",
    "                \n",
    "                self.W[i-1] += -epsilon * dW\n",
    "\n",
    "                # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "                dW += reg_lambda * self.W[i-1]\n",
    "                # Gradient descent parameter update\n",
    "                self.b[i-1] += -epsilon * db\n",
    "\n",
    "#             if print_loss and epoch % 1000 == 0:\n",
    "#             if print_loss:\n",
    "#                 print(\"Loss after iteration %i: %f\" %(epoch, self.calculate_loss(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mData loaded...\u001b[0m\n",
      "\u001b[32m569 data rows for 28 features...\u001b[0m\n",
      "\u001b[32mShuffling the dataset...\u001b[0m\n",
      "\u001b[32m455 rows for the train dataset (80%), 114 rows for validation...\u001b[0m\n",
      "\n",
      "ff size z, a, mul (455, 3) (455, 3) (455, 3)\n",
      "ff size z, a, mul (455, 2) (455, 2) (455, 2)\n",
      "[[ 0.5653007   0.4346993 ]\n",
      " [ 0.55905189  0.44094811]] [0 1]\n",
      "dtahn shape (455, 2) [[-0.4346993   0.4346993 ]\n",
      " [ 0.55905189 -0.55905189]]\n",
      "ff size z, a, mul (455, 3) (455, 3) (455, 3)\n",
      "ff size z, a, mul (455, 2) (455, 2) (455, 2)\n",
      "[[ 0.6482587   0.3517413 ]\n",
      " [ 0.62020073  0.37979927]] [0 1]\n",
      "dtahn shape (455, 2) [[-0.3517413   0.3517413 ]\n",
      " [ 0.62020073 -0.62020073]]\n",
      "ff size z, a, mul (455, 3) (455, 3) (455, 3)\n",
      "ff size z, a, mul (455, 2) (455, 2) (455, 2)\n",
      "[[ 0.64786643  0.35213357]\n",
      " [ 0.59262402  0.40737598]] [0 1]\n",
      "dtahn shape (455, 2) [[-0.35213357  0.35213357]\n",
      " [ 0.59262402 -0.59262402]]\n",
      "ff size z, a, mul (455, 3) (455, 3) (455, 3)\n",
      "ff size z, a, mul (455, 2) (455, 2) (455, 2)\n",
      "[[ 0.79257478  0.20742522]\n",
      " [ 0.68520329  0.31479671]] [0 1]\n",
      "dtahn shape (455, 2) [[-0.20742522  0.20742522]\n",
      " [ 0.68520329 -0.68520329]]\n",
      "ff size z, a, mul (455, 3) (455, 3) (455, 3)\n",
      "ff size z, a, mul (455, 2) (455, 2) (455, 2)\n",
      "[[ 0.48274421  0.51725579]\n",
      " [ 0.48121002  0.51878998]] [0 1]\n",
      "dtahn shape (455, 2) [[-0.51725579  0.51725579]\n",
      " [ 0.48121002 -0.48121002]]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "def load_and_prep_data(csvfile):\n",
    "\n",
    "\t# category to int function for y\n",
    "\tdef f(i):\n",
    "\t\tif i[1] == 'M':\n",
    "\t\t\treturn 1\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\t#open file proc\n",
    "\tdef load_data(csvfile):\n",
    "\t\tif not os.path.isfile(csvfile):\n",
    "\t\t\texit_error('can\\'t find the file ' + csvfile)\n",
    "\t\tdata = []\n",
    "\t\twith open(csvfile) as csv_iterator:\n",
    "\t\t\tdata_reader = csv.reader(csv_iterator, delimiter=',')\n",
    "\t\t\tfor row in data_reader:\n",
    "\t\t\t\tdata.append(row)\n",
    "\t\tcsv_iterator.close()\n",
    "\t\tif len(data) < 1:\n",
    "\t\t\texit_error('file ' + csvfile + ' is empty')\n",
    "\t\treturn data\n",
    "\n",
    "\t# load data from csvfile\n",
    "\tdataRaw = np.array(load_data(csvfile))\n",
    "\tdataTemp = []\n",
    "\n",
    "\t# fill y / replace categorical values with numeric values (1 is for 'M')\n",
    "\ty = np.array([f(i) for i in dataRaw])\n",
    "\n",
    "\t# remove unwanted columns/features\n",
    "\tdataRaw = np.delete(dataRaw, [0,1,4,5], 1)\n",
    "\n",
    "\t# cast to float\n",
    "\tdataRaw = dataRaw.astype('float')\n",
    "\n",
    "\t# normalize data using transpose\n",
    "\tdataTemp = np.zeros((dataRaw.shape[1], dataRaw.shape[0]))\n",
    "\tfor index, feature in enumerate(dataRaw.T):\n",
    "\t\tdataTemp[index] = [(x - min(feature)) / (max(feature) - min(feature)) for x in feature]\n",
    "\t\n",
    "\tprint('\\n\\033[32mData loaded...\\033[0m')\n",
    "\tprint('\\033[32m%d data rows for %d features...\\033[0m' % (dataTemp.T.shape[0], dataTemp.T.shape[1]))\n",
    "\treturn dataTemp.T, y\n",
    "\n",
    "def divide_dataset(data, y, train_share):\n",
    "\tlimit = int(len(data) * train_share)\n",
    "\tp = np.random.permutation(len(data))\n",
    "\tdata = data[p]\n",
    "\ty = y[p]\n",
    "\tprint('\\033[32mShuffling the dataset...\\033[0m')\n",
    "\treturn data[:limit], data[limit:], y[:limit], y[limit:]\n",
    "\n",
    "np.random.seed(42)\n",
    "train_share = 0.8\t\t\t#share of the dataset to use as train set\n",
    "mlp_layers = [10,20]\t\t#size of each hidden layer\n",
    "mlp_init = ''\t\t\t\t#random sur distrib 'uniform' or 'normal'(default normal)\n",
    "mlp_activation = ''\t\t\t#'relu' (rectified linear unit) or 'sigmoid' or 'tanh'(hyperboloid tangent) (default tanh)\n",
    "nb_cats = 2\t\t\t\t\t#size of the output layer\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "learningR = 0.01\n",
    "\n",
    "csvfile = './data/data.csv'\n",
    "# Data retrieval and cleaning\n",
    "data, y = load_and_prep_data(csvfile)\n",
    "\n",
    "# Creation of train and validation dataset\n",
    "x_train, x_valid, y_train, y_valid = divide_dataset(data, y, train_share)\n",
    "batch_size = x_train.shape[0]\n",
    "print('\\033[32m%d rows for the train dataset (%d%%), %d rows for validation...\\033[0m\\n' % \\\n",
    "    (x_train.shape[0], train_share * 100, x_valid.shape[0]))\n",
    "\n",
    "layers_dim = [28, 3, 2]\n",
    "\n",
    "model = Model(layers_dim)\n",
    "model.train(x_train, y_train, num_passes=5, epsilon=0.01, reg_lambda=0.01, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to plot a decision boundary.\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.74346118  0.46465633]\n",
      " [ 1.65755662 -0.63203157]]\n",
      "[[ 0.87401034  0.12598966]\n",
      " [ 0.18482108  0.81517892]\n",
      " [ 0.88046299  0.11953701]\n",
      " [ 0.88046298  0.11953702]\n",
      " [ 0.23538554  0.76461446]] [0 1 1 0 1]\n",
      "Loss after iteration 0: 0.596832\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "# plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "# plt.show()\n",
    "print(X[:2])\n",
    "layers_dim = [2, 30, 2]\n",
    "\n",
    "model = Model(layers_dim)\n",
    "model.train(X, y, num_passes=1000, epsilon=0.01, reg_lambda=0.01, print_loss=True)\n",
    "\n",
    "# # Plot the decision boundary\n",
    "# plot_decision_boundary(lambda x: model.predict(x), X, y)\n",
    "# plt.title(\"Decision Boundary for hidden layer size 3\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
