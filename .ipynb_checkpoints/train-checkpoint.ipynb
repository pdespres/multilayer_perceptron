{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MultiplyGate:\n",
    "    def forward(self,W, X):\n",
    "        return np.dot(X, W)\n",
    "\n",
    "    def backward(self, W, X, dZ):\n",
    "        dW = np.dot(np.transpose(X), dZ)\n",
    "        dX = np.dot(dZ, np.transpose(W))\n",
    "        return dW, dX\n",
    "\n",
    "class AddGate:\n",
    "    def forward(self, X, b):\n",
    "        return X + b\n",
    "\n",
    "    def backward(self, X, b, dZ):\n",
    "        dX = dZ * np.ones_like(X)\n",
    "        db = np.dot(np.ones((1, dZ.shape[0]), dtype=np.float64), dZ)\n",
    "        return db, dX\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, X):\n",
    "        return 1.0 / (1.0 + np.exp(-X))\n",
    "\n",
    "    def backward(self, X, top_diff):\n",
    "        output = self.forward(X)\n",
    "        return (1.0 - output) * output * top_diff\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, X):\n",
    "        return np.tanh(X)\n",
    "\n",
    "    def backward(self, X, top_diff):\n",
    "        output = self.forward(X)\n",
    "        return (1.0 - np.square(output)) * top_diff\n",
    "    \n",
    "class Softmax:\n",
    "    def predict(self, X):\n",
    "        exp_scores = np.exp(X)\n",
    "        return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "    def loss(self, X, y):\n",
    "        num_examples = X.shape[0]\n",
    "        probs = self.predict(X)\n",
    "        print(probs[:5], y[:5])\n",
    "        corect_logprobs = -np.log(probs[range(num_examples), y])\n",
    "        data_loss = np.sum(corect_logprobs)\n",
    "#         print('data loss', 1./num_examples * data_loss)\n",
    "        return 1./num_examples * data_loss\n",
    "#         log_likelihood = -np.log(probs[range(y.shape[0]), y])\n",
    "#         print('loss', np.sum(log_likelihood) / y.shape[0])\n",
    "#         loss = np.sum(log_likelihood) / y.shape[0]\n",
    "# #         return loss\n",
    "#         return 1./num_examples * data_loss\n",
    "    \n",
    "    def diff(self, X, y):\n",
    "        num_examples = X.shape[0]\n",
    "        probs = self.predict(X)\n",
    "        probs[range(num_examples), y] -= 1\n",
    "        return probs\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, layers_dim):\n",
    "        self.b = []\n",
    "        self.W = []\n",
    "        for i in range(len(layers_dim)-1):\n",
    "            self.W.append(np.random.randn(layers_dim[i], layers_dim[i+1]) / np.sqrt(layers_dim[i]))\n",
    "            self.b.append(np.random.randn(layers_dim[i+1]).reshape(1, layers_dim[i+1]))\n",
    "#             self.W.append(np.zeros(layers_dim[i], layers_dim[i+1]))\n",
    "#             self.b.append(np.ones(layers_dim[i+1]).reshape(1, layers_dim[i+1]))\n",
    "\n",
    "    def predict(self, X):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "\n",
    "        input = X\n",
    "        for i in range(len(self.W)):\n",
    "            mul = mulGate.forward(self.W[i], input)\n",
    "            add = addGate.forward(mul, self.b[i])\n",
    "            input = layer.forward(add)\n",
    "\n",
    "        probs = softmaxOutput.predict(input)\n",
    "        return np.argmax(probs, axis=1)\n",
    "\n",
    "    def calculate_loss(self, X, y):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "\n",
    "        input = X\n",
    "        for i in range(len(self.W)):\n",
    "            mul = mulGate.forward(self.W[i], input)\n",
    "            add = addGate.forward(mul, self.b[i])\n",
    "            input = layer.forward(add)\n",
    "\n",
    "        return softmaxOutput.loss(input, y)\n",
    "\n",
    "    def train(self, X, y, num_passes=70, epsilon=0.01, reg_lambda=0.01, print_loss=False):\n",
    "        mulGate = MultiplyGate()\n",
    "        addGate = AddGate()\n",
    "        layer = Tanh()\n",
    "        softmaxOutput = Softmax()\n",
    "\n",
    "        for epoch in range(num_passes):\n",
    "            # Forward propagation\n",
    "            input = X\n",
    "            forward = [(None, None, input)]\n",
    "            for i in range(len(self.W)):\n",
    "                mul = mulGate.forward(self.W[i], input)\n",
    "                add = addGate.forward(mul, self.b[i])\n",
    "                input = layer.forward(add)\n",
    "                forward.append((mul, add, input))\n",
    "#                 print('ff size z, a, mul', add.shape, input.shape, mul.shape)\n",
    "\n",
    "            # Back propagation\n",
    "            dtanh = softmaxOutput.diff(forward[len(forward)-1][2], y)\n",
    "#             print('dtanh init', dtanh.shape)\n",
    "            for i in range(len(forward)-1, 0, -1):\n",
    "                dadd = layer.backward(forward[i][1], dtanh)\n",
    "                db, dmul = addGate.backward(forward[i][0], self.b[i-1], dadd)\n",
    "                dW, dtanh = mulGate.backward(self.W[i-1], forward[i-1][2], dmul)\n",
    "                \n",
    "                self.W[i-1] += -epsilon * dW\n",
    "\n",
    "                # Add regularization terms (b1 and b2 don't have regularization terms)\n",
    "                dW += reg_lambda * self.W[i-1]\n",
    "                # Gradient descent parameter update\n",
    "                self.b[i-1] += -epsilon * db\n",
    "\n",
    "            if print_loss and epoch % 1000 == 0:\n",
    "#             if print_loss:\n",
    "                print(\"Loss after iteration %i: %f\" %(epoch, self.calculate_loss(X, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mData loaded...\u001b[0m\n",
      "\u001b[32m569 data rows for 28 features...\u001b[0m\n",
      "\u001b[32mShuffling the dataset...\u001b[0m\n",
      "\u001b[32m455 rows for the train dataset (80%), 114 rows for validation...\u001b[0m\n",
      "\n",
      "[[ 0.52298337  0.47701663]\n",
      " [ 0.53404563  0.46595437]\n",
      " [ 0.53037174  0.46962826]\n",
      " [ 0.52198885  0.47801115]\n",
      " [ 0.51709471  0.48290529]] [0 1 1 0 0]\n",
      "[[-0.47701663  0.47701663]\n",
      " [ 0.53404563 -0.53404563]\n",
      " [ 0.53037174 -0.53037174]\n",
      " [-0.47801115  0.47801115]\n",
      " [-0.48290529  0.48290529]]\n",
      "Loss after iteration 0: 0.811482\n",
      "[[ 0.86551733  0.13448267]\n",
      " [ 0.86549615  0.13450385]\n",
      " [ 0.86443273  0.13556727]\n",
      " [ 0.86514741  0.13485259]\n",
      " [ 0.86653003  0.13346997]] [0 1 1 0 0]\n",
      "[[-0.13448267  0.13448267]\n",
      " [ 0.86549615 -0.86549615]\n",
      " [ 0.86443273 -0.86443273]\n",
      " [-0.13485259  0.13485259]\n",
      " [-0.13346997  0.13346997]]\n",
      "Loss after iteration 1: 0.691014\n",
      "[[ 0.50382205  0.49617795]\n",
      " [ 0.50379994  0.49620006]\n",
      " [ 0.50380424  0.49619576]\n",
      " [ 0.50381723  0.49618277]\n",
      " [ 0.50385128  0.49614872]] [0 1 1 0 0]\n",
      "[[-0.49617795  0.49617795]\n",
      " [ 0.50379994 -0.50379994]\n",
      " [ 0.50380424 -0.50380424]\n",
      " [-0.49618277  0.49618277]\n",
      " [-0.49614872  0.49614872]]\n",
      "Loss after iteration 2: 0.690077\n",
      "[[ 0.50549497  0.49450503]\n",
      " [ 0.50545685  0.49454315]\n",
      " [ 0.50546175  0.49453825]\n",
      " [ 0.5054915   0.4945085 ]\n",
      " [ 0.50554644  0.49445356]] [0 1 1 0 0]\n",
      "[[-0.49450503  0.49450503]\n",
      " [ 0.50545685 -0.50545685]\n",
      " [ 0.50546175 -0.50546175]\n",
      " [-0.4945085   0.4945085 ]\n",
      " [-0.49445356  0.49445356]]\n",
      "Loss after iteration 3: 0.688053\n",
      "[[ 0.50914458  0.49085542]\n",
      " [ 0.50906675  0.49093325]\n",
      " [ 0.50907147  0.49092853]\n",
      " [ 0.50914693  0.49085307]\n",
      " [ 0.50925252  0.49074748]] [0 1 1 0 0]\n",
      "[[-0.49085542  0.49085542]\n",
      " [ 0.50906675 -0.50906675]\n",
      " [ 0.50907147 -0.50907147]\n",
      " [-0.49085307  0.49085307]\n",
      " [-0.49074748  0.49074748]]\n",
      "Loss after iteration 4: 0.682049\n",
      "[[ 0.52043115  0.47956885]\n",
      " [ 0.52021086  0.47978914]\n",
      " [ 0.52020697  0.47979303]\n",
      " [ 0.52046441  0.47953559]\n",
      " [ 0.52074822  0.47925178]] [0 1 1 0 0]\n",
      "[[-0.47956885  0.47956885]\n",
      " [ 0.52021086 -0.52021086]\n",
      " [ 0.52020697 -0.52020697]\n",
      " [-0.47953559  0.47953559]\n",
      " [-0.47925178  0.47925178]]\n",
      "Loss after iteration 5: 0.655729\n",
      "[[ 0.58950503  0.41049497]\n",
      " [ 0.58835743  0.41164257]\n",
      " [ 0.58820772  0.41179228]\n",
      " [ 0.589827    0.410173  ]\n",
      " [ 0.59125112  0.40874888]] [0 1 1 0 0]\n",
      "[[-0.41049497  0.41049497]\n",
      " [ 0.58835743 -0.58835743]\n",
      " [ 0.58820772 -0.58820772]\n",
      " [-0.410173    0.410173  ]\n",
      " [-0.40874888  0.40874888]]\n",
      "Loss after iteration 6: 0.744309\n",
      "[[ 0.82561137  0.17438863]\n",
      " [ 0.8241706   0.1758294 ]\n",
      " [ 0.82354984  0.17645016]\n",
      " [ 0.82610375  0.17389625]\n",
      " [ 0.82803048  0.17196952]] [0 1 1 0 0]\n",
      "[[-0.17438863  0.17438863]\n",
      " [ 0.8241706  -0.8241706 ]\n",
      " [ 0.82354984 -0.82354984]\n",
      " [-0.17389625  0.17389625]\n",
      " [-0.17196952  0.17196952]]\n",
      "Loss after iteration 7: 0.693162\n",
      "[[ 0.4999334   0.5000666 ]\n",
      " [ 0.49992328  0.50007672]\n",
      " [ 0.49992489  0.50007511]\n",
      " [ 0.49993906  0.50006094]\n",
      " [ 0.49994171  0.50005829]] [0 1 1 0 0]\n",
      "[[-0.5000666   0.5000666 ]\n",
      " [ 0.49992328 -0.49992328]\n",
      " [ 0.49992489 -0.49992489]\n",
      " [-0.50006094  0.50006094]\n",
      " [-0.50005829  0.50005829]]\n",
      "Loss after iteration 8: 0.693162\n",
      "[[ 0.49993418  0.50006582]\n",
      " [ 0.49992413  0.50007587]\n",
      " [ 0.49992572  0.50007428]\n",
      " [ 0.49993979  0.50006021]\n",
      " [ 0.49994243  0.50005757]] [0 1 1 0 0]\n",
      "[[-0.50006582  0.50006582]\n",
      " [ 0.49992413 -0.49992413]\n",
      " [ 0.49992572 -0.49992572]\n",
      " [-0.50006021  0.50006021]\n",
      " [-0.50005757  0.50005757]]\n",
      "Loss after iteration 9: 0.693161\n",
      "[[ 0.49993494  0.50006506]\n",
      " [ 0.49992496  0.50007504]\n",
      " [ 0.49992654  0.50007346]\n",
      " [ 0.49994051  0.50005949]\n",
      " [ 0.49994314  0.50005686]] [0 1 1 0 0]\n",
      "[[-0.50006506  0.50006506]\n",
      " [ 0.49992496 -0.49992496]\n",
      " [ 0.49992654 -0.49992654]\n",
      " [-0.50005949  0.50005949]\n",
      " [-0.50005686  0.50005686]]\n",
      "Loss after iteration 10: 0.693161\n",
      "[[ 0.49993569  0.50006431]\n",
      " [ 0.49992578  0.50007422]\n",
      " [ 0.49992735  0.50007265]\n",
      " [ 0.49994122  0.50005878]\n",
      " [ 0.49994384  0.50005616]] [0 1 1 0 0]\n",
      "[[-0.50006431  0.50006431]\n",
      " [ 0.49992578 -0.49992578]\n",
      " [ 0.49992735 -0.49992735]\n",
      " [-0.50005878  0.50005878]\n",
      " [-0.50005616  0.50005616]]\n",
      "Loss after iteration 11: 0.693161\n",
      "[[ 0.49993643  0.50006357]\n",
      " [ 0.49992659  0.50007341]\n",
      " [ 0.49992814  0.50007186]\n",
      " [ 0.49994192  0.50005808]\n",
      " [ 0.49994452  0.50005548]] [0 1 1 0 0]\n",
      "[[-0.50006357  0.50006357]\n",
      " [ 0.49992659 -0.49992659]\n",
      " [ 0.49992814 -0.49992814]\n",
      " [-0.50005808  0.50005808]\n",
      " [-0.50005548  0.50005548]]\n",
      "Loss after iteration 12: 0.693161\n",
      "[[ 0.49993716  0.50006284]\n",
      " [ 0.49992738  0.50007262]\n",
      " [ 0.49992892  0.50007108]\n",
      " [ 0.49994261  0.50005739]\n",
      " [ 0.4999452   0.5000548 ]] [0 1 1 0 0]\n",
      "[[-0.50006284  0.50006284]\n",
      " [ 0.49992738 -0.49992738]\n",
      " [ 0.49992892 -0.49992892]\n",
      " [-0.50005739  0.50005739]\n",
      " [-0.5000548   0.5000548 ]]\n",
      "Loss after iteration 13: 0.693160\n",
      "[[ 0.49993788  0.50006212]\n",
      " [ 0.49992817  0.50007183]\n",
      " [ 0.49992969  0.50007031]\n",
      " [ 0.49994329  0.50005671]\n",
      " [ 0.49994586  0.50005414]] [0 1 1 0 0]\n",
      "[[-0.50006212  0.50006212]\n",
      " [ 0.49992817 -0.49992817]\n",
      " [ 0.49992969 -0.49992969]\n",
      " [-0.50005671  0.50005671]\n",
      " [-0.50005414  0.50005414]]\n",
      "Loss after iteration 14: 0.693160\n",
      "[[ 0.49993859  0.50006141]\n",
      " [ 0.49992894  0.50007106]\n",
      " [ 0.49993045  0.50006955]\n",
      " [ 0.49994395  0.50005605]\n",
      " [ 0.49994652  0.50005348]] [0 1 1 0 0]\n",
      "[[-0.50006141  0.50006141]\n",
      " [ 0.49992894 -0.49992894]\n",
      " [ 0.49993045 -0.49993045]\n",
      " [-0.50005605  0.50005605]\n",
      " [-0.50005348  0.50005348]]\n",
      "Loss after iteration 15: 0.693160\n",
      "[[ 0.49993928  0.50006072]\n",
      " [ 0.4999297   0.5000703 ]\n",
      " [ 0.4999312   0.5000688 ]\n",
      " [ 0.49994461  0.50005539]\n",
      " [ 0.49994716  0.50005284]] [0 1 1 0 0]\n",
      "[[-0.50006072  0.50006072]\n",
      " [ 0.4999297  -0.4999297 ]\n",
      " [ 0.4999312  -0.4999312 ]\n",
      " [-0.50005539  0.50005539]\n",
      " [-0.50005284  0.50005284]]\n",
      "Loss after iteration 16: 0.693160\n",
      "[[ 0.49993997  0.50006003]\n",
      " [ 0.49993045  0.50006955]\n",
      " [ 0.49993193  0.50006807]\n",
      " [ 0.49994526  0.50005474]\n",
      " [ 0.4999478   0.5000522 ]] [0 1 1 0 0]\n",
      "[[-0.50006003  0.50006003]\n",
      " [ 0.49993045 -0.49993045]\n",
      " [ 0.49993193 -0.49993193]\n",
      " [-0.50005474  0.50005474]\n",
      " [-0.5000522   0.5000522 ]]\n",
      "Loss after iteration 17: 0.693159\n",
      "[[ 0.49994064  0.50005936]\n",
      " [ 0.49993119  0.50006881]\n",
      " [ 0.49993266  0.50006734]\n",
      " [ 0.4999459   0.5000541 ]\n",
      " [ 0.49994843  0.50005157]] [0 1 1 0 0]\n",
      "[[-0.50005936  0.50005936]\n",
      " [ 0.49993119 -0.49993119]\n",
      " [ 0.49993266 -0.49993266]\n",
      " [-0.5000541   0.5000541 ]\n",
      " [-0.50005157  0.50005157]]\n",
      "Loss after iteration 18: 0.693159\n",
      "[[ 0.49994131  0.50005869]\n",
      " [ 0.49993191  0.50006809]\n",
      " [ 0.49993338  0.50006662]\n",
      " [ 0.49994652  0.50005348]\n",
      " [ 0.49994904  0.50005096]] [0 1 1 0 0]\n",
      "[[-0.50005869  0.50005869]\n",
      " [ 0.49993191 -0.49993191]\n",
      " [ 0.49993338 -0.49993338]\n",
      " [-0.50005348  0.50005348]\n",
      " [-0.50005096  0.50005096]]\n",
      "Loss after iteration 19: 0.693159\n",
      "[[ 0.49994197  0.50005803]\n",
      " [ 0.49993263  0.50006737]\n",
      " [ 0.49993408  0.50006592]\n",
      " [ 0.49994714  0.50005286]\n",
      " [ 0.49994965  0.50005035]] [0 1 1 0 0]\n",
      "[[-0.50005803  0.50005803]\n",
      " [ 0.49993263 -0.49993263]\n",
      " [ 0.49993408 -0.49993408]\n",
      " [-0.50005286  0.50005286]\n",
      " [-0.50005035  0.50005035]]\n",
      "Loss after iteration 20: 0.693159\n",
      "[[ 0.49994262  0.50005738]\n",
      " [ 0.49993334  0.50006666]\n",
      " [ 0.49993478  0.50006522]\n",
      " [ 0.49994776  0.50005224]\n",
      " [ 0.49995025  0.50004975]] [0 1 1 0 0]\n",
      "[[-0.50005738  0.50005738]\n",
      " [ 0.49993334 -0.49993334]\n",
      " [ 0.49993478 -0.49993478]\n",
      " [-0.50005224  0.50005224]\n",
      " [-0.50004975  0.50004975]]\n",
      "Loss after iteration 21: 0.693158\n",
      "[[ 0.49994325  0.50005675]\n",
      " [ 0.49993404  0.50006596]\n",
      " [ 0.49993546  0.50006454]\n",
      " [ 0.49994836  0.50005164]\n",
      " [ 0.49995084  0.50004916]] [0 1 1 0 0]\n",
      "[[-0.50005675  0.50005675]\n",
      " [ 0.49993404 -0.49993404]\n",
      " [ 0.49993546 -0.49993546]\n",
      " [-0.50005164  0.50005164]\n",
      " [-0.50004916  0.50004916]]\n",
      "Loss after iteration 22: 0.693158\n",
      "[[ 0.49994388  0.50005612]\n",
      " [ 0.49993472  0.50006528]\n",
      " [ 0.49993614  0.50006386]\n",
      " [ 0.49994895  0.50005105]\n",
      " [ 0.49995143  0.50004857]] [0 1 1 0 0]\n",
      "[[-0.50005612  0.50005612]\n",
      " [ 0.49993472 -0.49993472]\n",
      " [ 0.49993614 -0.49993614]\n",
      " [-0.50005105  0.50005105]\n",
      " [-0.50004857  0.50004857]]\n",
      "Loss after iteration 23: 0.693158\n",
      "[[ 0.4999445   0.5000555 ]\n",
      " [ 0.4999354   0.5000646 ]\n",
      " [ 0.4999368   0.5000632 ]\n",
      " [ 0.49994954  0.50005046]\n",
      " [ 0.499952    0.500048  ]] [0 1 1 0 0]\n",
      "[[-0.5000555   0.5000555 ]\n",
      " [ 0.4999354  -0.4999354 ]\n",
      " [ 0.4999368  -0.4999368 ]\n",
      " [-0.50005046  0.50005046]\n",
      " [-0.500048    0.500048  ]]\n",
      "Loss after iteration 24: 0.693158\n",
      "[[ 0.49994512  0.50005488]\n",
      " [ 0.49993607  0.50006393]\n",
      " [ 0.49993746  0.50006254]\n",
      " [ 0.49995012  0.50004988]\n",
      " [ 0.49995257  0.50004743]] [0 1 1 0 0]\n",
      "[[-0.50005488  0.50005488]\n",
      " [ 0.49993607 -0.49993607]\n",
      " [ 0.49993746 -0.49993746]\n",
      " [-0.50004988  0.50004988]\n",
      " [-0.50004743  0.50004743]]\n",
      "Loss after iteration 25: 0.693157\n",
      "[[ 0.49994572  0.50005428]\n",
      " [ 0.49993673  0.50006327]\n",
      " [ 0.49993811  0.50006189]\n",
      " [ 0.49995069  0.50004931]\n",
      " [ 0.49995313  0.50004687]] [0 1 1 0 0]\n",
      "[[-0.50005428  0.50005428]\n",
      " [ 0.49993673 -0.49993673]\n",
      " [ 0.49993811 -0.49993811]\n",
      " [-0.50004931  0.50004931]\n",
      " [-0.50004687  0.50004687]]\n",
      "Loss after iteration 26: 0.693157\n",
      "[[ 0.49994632  0.50005368]\n",
      " [ 0.49993738  0.50006262]\n",
      " [ 0.49993875  0.50006125]\n",
      " [ 0.49995125  0.50004875]\n",
      " [ 0.49995369  0.50004631]] [0 1 1 0 0]\n",
      "[[-0.50005368  0.50005368]\n",
      " [ 0.49993738 -0.49993738]\n",
      " [ 0.49993875 -0.49993875]\n",
      " [-0.50004875  0.50004875]\n",
      " [-0.50004631  0.50004631]]\n",
      "Loss after iteration 27: 0.693157\n",
      "[[ 0.49994691  0.50005309]\n",
      " [ 0.49993803  0.50006197]\n",
      " [ 0.49993938  0.50006062]\n",
      " [ 0.49995181  0.50004819]\n",
      " [ 0.49995423  0.50004577]] [0 1 1 0 0]\n",
      "[[-0.50005309  0.50005309]\n",
      " [ 0.49993803 -0.49993803]\n",
      " [ 0.49993938 -0.49993938]\n",
      " [-0.50004819  0.50004819]\n",
      " [-0.50004577  0.50004577]]\n",
      "Loss after iteration 28: 0.693157\n",
      "[[ 0.49994749  0.50005251]\n",
      " [ 0.49993866  0.50006134]\n",
      " [ 0.49994001  0.50005999]\n",
      " [ 0.49995236  0.50004764]\n",
      " [ 0.49995477  0.50004523]] [0 1 1 0 0]\n",
      "[[-0.50005251  0.50005251]\n",
      " [ 0.49993866 -0.49993866]\n",
      " [ 0.49994001 -0.49994001]\n",
      " [-0.50004764  0.50004764]\n",
      " [-0.50004523  0.50004523]]\n",
      "Loss after iteration 29: 0.693157\n",
      "[[ 0.49994806  0.50005194]\n",
      " [ 0.49993929  0.50006071]\n",
      " [ 0.49994063  0.50005937]\n",
      " [ 0.4999529   0.5000471 ]\n",
      " [ 0.4999553   0.5000447 ]] [0 1 1 0 0]\n",
      "[[-0.50005194  0.50005194]\n",
      " [ 0.49993929 -0.49993929]\n",
      " [ 0.49994063 -0.49994063]\n",
      " [-0.5000471   0.5000471 ]\n",
      " [-0.5000447   0.5000447 ]]\n",
      "Loss after iteration 30: 0.693156\n",
      "[[ 0.49994863  0.50005137]\n",
      " [ 0.49993991  0.50006009]\n",
      " [ 0.49994123  0.50005877]\n",
      " [ 0.49995343  0.50004657]\n",
      " [ 0.49995583  0.50004417]] [0 1 1 0 0]\n",
      "[[-0.50005137  0.50005137]\n",
      " [ 0.49993991 -0.49993991]\n",
      " [ 0.49994123 -0.49994123]\n",
      " [-0.50004657  0.50004657]\n",
      " [-0.50004417  0.50004417]]\n",
      "Loss after iteration 31: 0.693156\n",
      "[[ 0.49994919  0.50005081]\n",
      " [ 0.49994052  0.50005948]\n",
      " [ 0.49994183  0.50005817]\n",
      " [ 0.49995396  0.50004604]\n",
      " [ 0.49995635  0.50004365]] [0 1 1 0 0]\n",
      "[[-0.50005081  0.50005081]\n",
      " [ 0.49994052 -0.49994052]\n",
      " [ 0.49994183 -0.49994183]\n",
      " [-0.50004604  0.50004604]\n",
      " [-0.50004365  0.50004365]]\n",
      "Loss after iteration 32: 0.693156\n",
      "[[ 0.49994974  0.50005026]\n",
      " [ 0.49994112  0.50005888]\n",
      " [ 0.49994243  0.50005757]\n",
      " [ 0.49995448  0.50004552]\n",
      " [ 0.49995686  0.50004314]] [0 1 1 0 0]\n",
      "[[-0.50005026  0.50005026]\n",
      " [ 0.49994112 -0.49994112]\n",
      " [ 0.49994243 -0.49994243]\n",
      " [-0.50004552  0.50004552]\n",
      " [-0.50004314  0.50004314]]\n",
      "Loss after iteration 33: 0.693156\n",
      "[[ 0.49995029  0.50004971]\n",
      " [ 0.49994172  0.50005828]\n",
      " [ 0.49994301  0.50005699]\n",
      " [ 0.499955    0.500045  ]\n",
      " [ 0.49995737  0.50004263]] [0 1 1 0 0]\n",
      "[[-0.50004971  0.50004971]\n",
      " [ 0.49994172 -0.49994172]\n",
      " [ 0.49994301 -0.49994301]\n",
      " [-0.500045    0.500045  ]\n",
      " [-0.50004263  0.50004263]]\n",
      "Loss after iteration 34: 0.693156\n",
      "[[ 0.49995083  0.50004917]\n",
      " [ 0.49994231  0.50005769]\n",
      " [ 0.49994359  0.50005641]\n",
      " [ 0.49995551  0.50004449]\n",
      " [ 0.49995787  0.50004213]] [0 1 1 0 0]\n",
      "[[-0.50004917  0.50004917]\n",
      " [ 0.49994231 -0.49994231]\n",
      " [ 0.49994359 -0.49994359]\n",
      " [-0.50004449  0.50004449]\n",
      " [-0.50004213  0.50004213]]\n",
      "Loss after iteration 35: 0.693155\n",
      "[[ 0.49995136  0.50004864]\n",
      " [ 0.49994289  0.50005711]\n",
      " [ 0.49994417  0.50005583]\n",
      " [ 0.49995601  0.50004399]\n",
      " [ 0.49995836  0.50004164]] [0 1 1 0 0]\n",
      "[[-0.50004864  0.50004864]\n",
      " [ 0.49994289 -0.49994289]\n",
      " [ 0.49994417 -0.49994417]\n",
      " [-0.50004399  0.50004399]\n",
      " [-0.50004164  0.50004164]]\n",
      "Loss after iteration 36: 0.693155\n",
      "[[ 0.49995189  0.50004811]\n",
      " [ 0.49994347  0.50005653]\n",
      " [ 0.49994473  0.50005527]\n",
      " [ 0.49995651  0.50004349]\n",
      " [ 0.49995885  0.50004115]] [0 1 1 0 0]\n",
      "[[-0.50004811  0.50004811]\n",
      " [ 0.49994347 -0.49994347]\n",
      " [ 0.49994473 -0.49994473]\n",
      " [-0.50004349  0.50004349]\n",
      " [-0.50004115  0.50004115]]\n",
      "Loss after iteration 37: 0.693155\n",
      "[[ 0.49995241  0.50004759]\n",
      " [ 0.49994404  0.50005596]\n",
      " [ 0.49994529  0.50005471]\n",
      " [ 0.499957    0.500043  ]\n",
      " [ 0.49995933  0.50004067]] [0 1 1 0 0]\n",
      "[[-0.50004759  0.50004759]\n",
      " [ 0.49994404 -0.49994404]\n",
      " [ 0.49994529 -0.49994529]\n",
      " [-0.500043    0.500043  ]\n",
      " [-0.50004067  0.50004067]]\n",
      "Loss after iteration 38: 0.693155\n",
      "[[ 0.49995292  0.50004708]\n",
      " [ 0.4999446   0.5000554 ]\n",
      " [ 0.49994584  0.50005416]\n",
      " [ 0.49995748  0.50004252]\n",
      " [ 0.49995981  0.50004019]] [0 1 1 0 0]\n",
      "[[-0.50004708  0.50004708]\n",
      " [ 0.4999446  -0.4999446 ]\n",
      " [ 0.49994584 -0.49994584]\n",
      " [-0.50004252  0.50004252]\n",
      " [-0.50004019  0.50004019]]\n",
      "Loss after iteration 39: 0.693155\n",
      "[[ 0.49995343  0.50004657]\n",
      " [ 0.49994515  0.50005485]\n",
      " [ 0.49994639  0.50005361]\n",
      " [ 0.49995796  0.50004204]\n",
      " [ 0.49996028  0.50003972]] [0 1 1 0 0]\n",
      "[[-0.50004657  0.50004657]\n",
      " [ 0.49994515 -0.49994515]\n",
      " [ 0.49994639 -0.49994639]\n",
      " [-0.50004204  0.50004204]\n",
      " [-0.50003972  0.50003972]]\n",
      "Loss after iteration 40: 0.693155\n",
      "[[ 0.49995393  0.50004607]\n",
      " [ 0.4999457   0.5000543 ]\n",
      " [ 0.49994693  0.50005307]\n",
      " [ 0.49995844  0.50004156]\n",
      " [ 0.49996075  0.50003925]] [0 1 1 0 0]\n",
      "[[-0.50004607  0.50004607]\n",
      " [ 0.4999457  -0.4999457 ]\n",
      " [ 0.49994693 -0.49994693]\n",
      " [-0.50004156  0.50004156]\n",
      " [-0.50003925  0.50003925]]\n",
      "Loss after iteration 41: 0.693154\n",
      "[[ 0.49995443  0.50004557]\n",
      " [ 0.49994625  0.50005375]\n",
      " [ 0.49994746  0.50005254]\n",
      " [ 0.49995891  0.50004109]\n",
      " [ 0.49996121  0.50003879]] [0 1 1 0 0]\n",
      "[[-0.50004557  0.50004557]\n",
      " [ 0.49994625 -0.49994625]\n",
      " [ 0.49994746 -0.49994746]\n",
      " [-0.50004109  0.50004109]\n",
      " [-0.50003879  0.50003879]]\n",
      "Loss after iteration 42: 0.693154\n",
      "[[ 0.49995492  0.50004508]\n",
      " [ 0.49994678  0.50005322]\n",
      " [ 0.49994799  0.50005201]\n",
      " [ 0.49995937  0.50004063]\n",
      " [ 0.49996167  0.50003833]] [0 1 1 0 0]\n",
      "[[-0.50004508  0.50004508]\n",
      " [ 0.49994678 -0.49994678]\n",
      " [ 0.49994799 -0.49994799]\n",
      " [-0.50004063  0.50004063]\n",
      " [-0.50003833  0.50003833]]\n",
      "Loss after iteration 43: 0.693154\n",
      "[[ 0.49995541  0.50004459]\n",
      " [ 0.49994731  0.50005269]\n",
      " [ 0.49994851  0.50005149]\n",
      " [ 0.49995983  0.50004017]\n",
      " [ 0.49996212  0.50003788]] [0 1 1 0 0]\n",
      "[[-0.50004459  0.50004459]\n",
      " [ 0.49994731 -0.49994731]\n",
      " [ 0.49994851 -0.49994851]\n",
      " [-0.50004017  0.50004017]\n",
      " [-0.50003788  0.50003788]]\n",
      "Loss after iteration 44: 0.693154\n",
      "[[ 0.49995589  0.50004411]\n",
      " [ 0.49994784  0.50005216]\n",
      " [ 0.49994903  0.50005097]\n",
      " [ 0.49996029  0.50003971]\n",
      " [ 0.49996257  0.50003743]] [0 1 1 0 0]\n",
      "[[-0.50004411  0.50004411]\n",
      " [ 0.49994784 -0.49994784]\n",
      " [ 0.49994903 -0.49994903]\n",
      " [-0.50003971  0.50003971]\n",
      " [-0.50003743  0.50003743]]\n",
      "Loss after iteration 45: 0.693154\n",
      "[[ 0.49995637  0.50004363]\n",
      " [ 0.49994836  0.50005164]\n",
      " [ 0.49994954  0.50005046]\n",
      " [ 0.49996074  0.50003926]\n",
      " [ 0.49996301  0.50003699]] [0 1 1 0 0]\n",
      "[[-0.50004363  0.50004363]\n",
      " [ 0.49994836 -0.49994836]\n",
      " [ 0.49994954 -0.49994954]\n",
      " [-0.50003926  0.50003926]\n",
      " [-0.50003699  0.50003699]]\n",
      "Loss after iteration 46: 0.693153\n",
      "[[ 0.49995684  0.50004316]\n",
      " [ 0.49994887  0.50005113]\n",
      " [ 0.49995005  0.50004995]\n",
      " [ 0.49996118  0.50003882]\n",
      " [ 0.49996345  0.50003655]] [0 1 1 0 0]\n",
      "[[-0.50004316  0.50004316]\n",
      " [ 0.49994887 -0.49994887]\n",
      " [ 0.49995005 -0.49995005]\n",
      " [-0.50003882  0.50003882]\n",
      " [-0.50003655  0.50003655]]\n",
      "Loss after iteration 47: 0.693153\n",
      "[[ 0.4999573   0.5000427 ]\n",
      " [ 0.49994938  0.50005062]\n",
      " [ 0.49995055  0.50004945]\n",
      " [ 0.49996162  0.50003838]\n",
      " [ 0.49996388  0.50003612]] [0 1 1 0 0]\n",
      "[[-0.5000427   0.5000427 ]\n",
      " [ 0.49994938 -0.49994938]\n",
      " [ 0.49995055 -0.49995055]\n",
      " [-0.50003838  0.50003838]\n",
      " [-0.50003612  0.50003612]]\n",
      "Loss after iteration 48: 0.693153\n",
      "[[ 0.49995777  0.50004223]\n",
      " [ 0.49994989  0.50005011]\n",
      " [ 0.49995104  0.50004896]\n",
      " [ 0.49996206  0.50003794]\n",
      " [ 0.49996431  0.50003569]] [0 1 1 0 0]\n",
      "[[-0.50004223  0.50004223]\n",
      " [ 0.49994989 -0.49994989]\n",
      " [ 0.49995104 -0.49995104]\n",
      " [-0.50003794  0.50003794]\n",
      " [-0.50003569  0.50003569]]\n",
      "Loss after iteration 49: 0.693153\n",
      "[[ 0.49995822  0.50004178]\n",
      " [ 0.49995038  0.50004962]\n",
      " [ 0.49995153  0.50004847]\n",
      " [ 0.49996249  0.50003751]\n",
      " [ 0.49996473  0.50003527]] [0 1 1 0 0]\n",
      "[[-0.50004178  0.50004178]\n",
      " [ 0.49995038 -0.49995038]\n",
      " [ 0.49995153 -0.49995153]\n",
      " [-0.50003751  0.50003751]\n",
      " [-0.50003527  0.50003527]]\n",
      "Loss after iteration 50: 0.693153\n",
      "[[ 0.49995868  0.50004132]\n",
      " [ 0.49995088  0.50004912]\n",
      " [ 0.49995202  0.50004798]\n",
      " [ 0.49996292  0.50003708]\n",
      " [ 0.49996515  0.50003485]] [0 1 1 0 0]\n",
      "[[-0.50004132  0.50004132]\n",
      " [ 0.49995088 -0.49995088]\n",
      " [ 0.49995202 -0.49995202]\n",
      " [-0.50003708  0.50003708]\n",
      " [-0.50003485  0.50003485]]\n",
      "Loss after iteration 51: 0.693153\n",
      "[[ 0.49995912  0.50004088]\n",
      " [ 0.49995137  0.50004863]\n",
      " [ 0.4999525   0.5000475 ]\n",
      " [ 0.49996334  0.50003666]\n",
      " [ 0.49996557  0.50003443]] [0 1 1 0 0]\n",
      "[[-0.50004088  0.50004088]\n",
      " [ 0.49995137 -0.49995137]\n",
      " [ 0.4999525  -0.4999525 ]\n",
      " [-0.50003666  0.50003666]\n",
      " [-0.50003443  0.50003443]]\n",
      "Loss after iteration 52: 0.693152\n",
      "[[ 0.49995957  0.50004043]\n",
      " [ 0.49995185  0.50004815]\n",
      " [ 0.49995297  0.50004703]\n",
      " [ 0.49996376  0.50003624]\n",
      " [ 0.49996598  0.50003402]] [0 1 1 0 0]\n",
      "[[-0.50004043  0.50004043]\n",
      " [ 0.49995185 -0.49995185]\n",
      " [ 0.49995297 -0.49995297]\n",
      " [-0.50003624  0.50003624]\n",
      " [-0.50003402  0.50003402]]\n",
      "Loss after iteration 53: 0.693152\n",
      "[[ 0.49996001  0.50003999]\n",
      " [ 0.49995233  0.50004767]\n",
      " [ 0.49995344  0.50004656]\n",
      " [ 0.49996417  0.50003583]\n",
      " [ 0.49996639  0.50003361]] [0 1 1 0 0]\n",
      "[[-0.50003999  0.50003999]\n",
      " [ 0.49995233 -0.49995233]\n",
      " [ 0.49995344 -0.49995344]\n",
      " [-0.50003583  0.50003583]\n",
      " [-0.50003361  0.50003361]]\n",
      "Loss after iteration 54: 0.693152\n",
      "[[ 0.49996044  0.50003956]\n",
      " [ 0.4999528   0.5000472 ]\n",
      " [ 0.49995391  0.50004609]\n",
      " [ 0.49996459  0.50003541]\n",
      " [ 0.49996679  0.50003321]] [0 1 1 0 0]\n",
      "[[-0.50003956  0.50003956]\n",
      " [ 0.4999528  -0.4999528 ]\n",
      " [ 0.49995391 -0.49995391]\n",
      " [-0.50003541  0.50003541]\n",
      " [-0.50003321  0.50003321]]\n",
      "Loss after iteration 55: 0.693152\n",
      "[[ 0.49996087  0.50003913]\n",
      " [ 0.49995327  0.50004673]\n",
      " [ 0.49995437  0.50004563]\n",
      " [ 0.49996499  0.50003501]\n",
      " [ 0.4999672   0.5000328 ]] [0 1 1 0 0]\n",
      "[[-0.50003913  0.50003913]\n",
      " [ 0.49995327 -0.49995327]\n",
      " [ 0.49995437 -0.49995437]\n",
      " [-0.50003501  0.50003501]\n",
      " [-0.5000328   0.5000328 ]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 56: 0.693152\n",
      "[[ 0.4999613   0.5000387 ]\n",
      " [ 0.49995374  0.50004626]\n",
      " [ 0.49995483  0.50004517]\n",
      " [ 0.4999654   0.5000346 ]\n",
      " [ 0.49996759  0.50003241]] [0 1 1 0 0]\n",
      "[[-0.5000387   0.5000387 ]\n",
      " [ 0.49995374 -0.49995374]\n",
      " [ 0.49995483 -0.49995483]\n",
      " [-0.5000346   0.5000346 ]\n",
      " [-0.50003241  0.50003241]]\n",
      "Loss after iteration 57: 0.693152\n",
      "[[ 0.49996172  0.50003828]\n",
      " [ 0.4999542   0.5000458 ]\n",
      " [ 0.49995528  0.50004472]\n",
      " [ 0.4999658   0.5000342 ]\n",
      " [ 0.49996799  0.50003201]] [0 1 1 0 0]\n",
      "[[-0.50003828  0.50003828]\n",
      " [ 0.4999542  -0.4999542 ]\n",
      " [ 0.49995528 -0.49995528]\n",
      " [-0.5000342   0.5000342 ]\n",
      " [-0.50003201  0.50003201]]\n",
      "Loss after iteration 58: 0.693152\n",
      "[[ 0.49996214  0.50003786]\n",
      " [ 0.49995466  0.50004534]\n",
      " [ 0.49995573  0.50004427]\n",
      " [ 0.49996619  0.50003381]\n",
      " [ 0.49996838  0.50003162]] [0 1 1 0 0]\n",
      "[[-0.50003786  0.50003786]\n",
      " [ 0.49995466 -0.49995466]\n",
      " [ 0.49995573 -0.49995573]\n",
      " [-0.50003381  0.50003381]\n",
      " [-0.50003162  0.50003162]]\n",
      "Loss after iteration 59: 0.693151\n",
      "[[ 0.49996255  0.50003745]\n",
      " [ 0.49995511  0.50004489]\n",
      " [ 0.49995618  0.50004382]\n",
      " [ 0.49996658  0.50003342]\n",
      " [ 0.49996876  0.50003124]] [0 1 1 0 0]\n",
      "[[-0.50003745  0.50003745]\n",
      " [ 0.49995511 -0.49995511]\n",
      " [ 0.49995618 -0.49995618]\n",
      " [-0.50003342  0.50003342]\n",
      " [-0.50003124  0.50003124]]\n",
      "Loss after iteration 60: 0.693151\n",
      "[[ 0.49996297  0.50003703]\n",
      " [ 0.49995556  0.50004444]\n",
      " [ 0.49995662  0.50004338]\n",
      " [ 0.49996697  0.50003303]\n",
      " [ 0.49996915  0.50003085]] [0 1 1 0 0]\n",
      "[[-0.50003703  0.50003703]\n",
      " [ 0.49995556 -0.49995556]\n",
      " [ 0.49995662 -0.49995662]\n",
      " [-0.50003303  0.50003303]\n",
      " [-0.50003085  0.50003085]]\n",
      "Loss after iteration 61: 0.693151\n",
      "[[ 0.49996337  0.50003663]\n",
      " [ 0.499956    0.500044  ]\n",
      " [ 0.49995706  0.50004294]\n",
      " [ 0.49996736  0.50003264]\n",
      " [ 0.49996953  0.50003047]] [0 1 1 0 0]\n",
      "[[-0.50003663  0.50003663]\n",
      " [ 0.499956   -0.499956  ]\n",
      " [ 0.49995706 -0.49995706]\n",
      " [-0.50003264  0.50003264]\n",
      " [-0.50003047  0.50003047]]\n",
      "Loss after iteration 62: 0.693151\n",
      "[[ 0.49996378  0.50003622]\n",
      " [ 0.49995644  0.50004356]\n",
      " [ 0.49995749  0.50004251]\n",
      " [ 0.49996774  0.50003226]\n",
      " [ 0.4999699   0.5000301 ]] [0 1 1 0 0]\n",
      "[[-0.50003622  0.50003622]\n",
      " [ 0.49995644 -0.49995644]\n",
      " [ 0.49995749 -0.49995749]\n",
      " [-0.50003226  0.50003226]\n",
      " [-0.5000301   0.5000301 ]]\n",
      "Loss after iteration 63: 0.693151\n",
      "[[ 0.49996418  0.50003582]\n",
      " [ 0.49995688  0.50004312]\n",
      " [ 0.49995792  0.50004208]\n",
      " [ 0.49996812  0.50003188]\n",
      " [ 0.49997028  0.50002972]] [0 1 1 0 0]\n",
      "[[-0.50003582  0.50003582]\n",
      " [ 0.49995688 -0.49995688]\n",
      " [ 0.49995792 -0.49995792]\n",
      " [-0.50003188  0.50003188]\n",
      " [-0.50002972  0.50002972]]\n",
      "Loss after iteration 64: 0.693151\n",
      "[[ 0.49996457  0.50003543]\n",
      " [ 0.49995731  0.50004269]\n",
      " [ 0.49995834  0.50004166]\n",
      " [ 0.4999685   0.5000315 ]\n",
      " [ 0.49997065  0.50002935]] [0 1 1 0 0]\n",
      "[[-0.50003543  0.50003543]\n",
      " [ 0.49995731 -0.49995731]\n",
      " [ 0.49995834 -0.49995834]\n",
      " [-0.5000315   0.5000315 ]\n",
      " [-0.50002935  0.50002935]]\n",
      "Loss after iteration 65: 0.693150\n",
      "[[ 0.49996497  0.50003503]\n",
      " [ 0.49995774  0.50004226]\n",
      " [ 0.49995877  0.50004123]\n",
      " [ 0.49996887  0.50003113]\n",
      " [ 0.49997101  0.50002899]] [0 1 1 0 0]\n",
      "[[-0.50003503  0.50003503]\n",
      " [ 0.49995774 -0.49995774]\n",
      " [ 0.49995877 -0.49995877]\n",
      " [-0.50003113  0.50003113]\n",
      " [-0.50002899  0.50002899]]\n",
      "Loss after iteration 66: 0.693150\n",
      "[[ 0.49996536  0.50003464]\n",
      " [ 0.49995817  0.50004183]\n",
      " [ 0.49995919  0.50004081]\n",
      " [ 0.49996924  0.50003076]\n",
      " [ 0.49997138  0.50002862]] [0 1 1 0 0]\n",
      "[[-0.50003464  0.50003464]\n",
      " [ 0.49995817 -0.49995817]\n",
      " [ 0.49995919 -0.49995919]\n",
      " [-0.50003076  0.50003076]\n",
      " [-0.50002862  0.50002862]]\n",
      "Loss after iteration 67: 0.693150\n",
      "[[ 0.49996575  0.50003425]\n",
      " [ 0.49995859  0.50004141]\n",
      " [ 0.4999596   0.5000404 ]\n",
      " [ 0.49996961  0.50003039]\n",
      " [ 0.49997174  0.50002826]] [0 1 1 0 0]\n",
      "[[-0.50003425  0.50003425]\n",
      " [ 0.49995859 -0.49995859]\n",
      " [ 0.4999596  -0.4999596 ]\n",
      " [-0.50003039  0.50003039]\n",
      " [-0.50002826  0.50002826]]\n",
      "Loss after iteration 68: 0.693150\n",
      "[[ 0.49996613  0.50003387]\n",
      " [ 0.49995901  0.50004099]\n",
      " [ 0.49996001  0.50003999]\n",
      " [ 0.49996997  0.50003003]\n",
      " [ 0.4999721   0.5000279 ]] [0 1 1 0 0]\n",
      "[[-0.50003387  0.50003387]\n",
      " [ 0.49995901 -0.49995901]\n",
      " [ 0.49996001 -0.49996001]\n",
      " [-0.50003003  0.50003003]\n",
      " [-0.5000279   0.5000279 ]]\n",
      "Loss after iteration 69: 0.693150\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import csv\n",
    "import os.path\n",
    "\n",
    "def load_and_prep_data(csvfile):\n",
    "\n",
    "\t# category to int function for y\n",
    "\tdef f(i):\n",
    "\t\tif i[1] == 'M':\n",
    "\t\t\treturn 1\n",
    "\t\telse:\n",
    "\t\t\treturn 0\n",
    "\n",
    "\t#open file proc\n",
    "\tdef load_data(csvfile):\n",
    "\t\tif not os.path.isfile(csvfile):\n",
    "\t\t\texit_error('can\\'t find the file ' + csvfile)\n",
    "\t\tdata = []\n",
    "\t\twith open(csvfile) as csv_iterator:\n",
    "\t\t\tdata_reader = csv.reader(csv_iterator, delimiter=',')\n",
    "\t\t\tfor row in data_reader:\n",
    "\t\t\t\tdata.append(row)\n",
    "\t\tcsv_iterator.close()\n",
    "\t\tif len(data) < 1:\n",
    "\t\t\texit_error('file ' + csvfile + ' is empty')\n",
    "\t\treturn data\n",
    "\n",
    "\t# load data from csvfile\n",
    "\tdataRaw = np.array(load_data(csvfile))\n",
    "\tdataTemp = []\n",
    "\n",
    "\t# fill y / replace categorical values with numeric values (1 is for 'M')\n",
    "\ty = np.array([f(i) for i in dataRaw])\n",
    "\n",
    "\t# remove unwanted columns/features\n",
    "\tdataRaw = np.delete(dataRaw, [0,1,4,5], 1)\n",
    "\n",
    "\t# cast to float\n",
    "\tdataRaw = dataRaw.astype('float')\n",
    "\n",
    "\t# normalize data using transpose\n",
    "\tdataTemp = np.zeros((dataRaw.shape[1], dataRaw.shape[0]))\n",
    "\tfor index, feature in enumerate(dataRaw.T):\n",
    "\t\tdataTemp[index] = [(x - min(feature)) / (max(feature) - min(feature)) for x in feature]\n",
    "\t\n",
    "\tprint('\\n\\033[32mData loaded...\\033[0m')\n",
    "\tprint('\\033[32m%d data rows for %d features...\\033[0m' % (dataTemp.T.shape[0], dataTemp.T.shape[1]))\n",
    "\treturn dataTemp.T, y\n",
    "\n",
    "def divide_dataset(data, y, train_share):\n",
    "\tlimit = int(len(data) * train_share)\n",
    "\tp = np.random.permutation(len(data))\n",
    "\tdata = data[p]\n",
    "\ty = y[p]\n",
    "\tprint('\\033[32mShuffling the dataset...\\033[0m')\n",
    "\treturn data[:limit], data[limit:], y[:limit], y[limit:]\n",
    "\n",
    "np.random.seed(42)\n",
    "train_share = 0.8\t\t\t#share of the dataset to use as train set\n",
    "mlp_layers = [10,20]\t\t#size of each hidden layer\n",
    "mlp_init = ''\t\t\t\t#random sur distrib 'uniform' or 'normal'(default normal)\n",
    "mlp_activation = ''\t\t\t#'relu' (rectified linear unit) or 'sigmoid' or 'tanh'(hyperboloid tangent) (default tanh)\n",
    "nb_cats = 2\t\t\t\t\t#size of the output layer\n",
    "epochs = 3\n",
    "batch_size = 64\n",
    "learningR = 0.01\n",
    "\n",
    "csvfile = './data/data.csv'\n",
    "# Data retrieval and cleaning\n",
    "data, y = load_and_prep_data(csvfile)\n",
    "\n",
    "# Creation of train and validation dataset\n",
    "x_train, x_valid, y_train, y_valid = divide_dataset(data, y, train_share)\n",
    "batch_size = x_train.shape[0]\n",
    "print('\\033[32m%d rows for the train dataset (%d%%), %d rows for validation...\\033[0m\\n' % \\\n",
    "    (x_train.shape[0], train_share * 100, x_valid.shape[0]))\n",
    "\n",
    "layers_dim = [28, 30, 10, 2]\n",
    "\n",
    "model = Model(layers_dim)\n",
    "model.train(x_train, y_train, num_passes=70, epsilon=0.01, reg_lambda=0.01, print_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Helper function to plot a decision boundary.\n",
    "def plot_decision_boundary(pred_func, X, y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.63543161  0.36456839]\n",
      " [ 0.69455111  0.30544889]\n",
      " [ 0.62039558  0.37960442]\n",
      " [ 0.46777197  0.53222803]\n",
      " [ 0.67857082  0.32142918]] [0 1 1 0 1]\n",
      "[[-0.36456839  0.36456839]\n",
      " [ 0.69455111 -0.69455111]\n",
      " [ 0.62039558 -0.62039558]\n",
      " [-0.53222803  0.53222803]\n",
      " [ 0.67857082 -0.67857082]]\n",
      "Loss after iteration 0: 0.700174\n",
      "[[ 0.53317122  0.46682878]\n",
      " [ 0.54034963  0.45965037]\n",
      " [ 0.52750566  0.47249434]\n",
      " [ 0.4129937   0.5870063 ]\n",
      " [ 0.53836556  0.46163444]] [0 1 1 0 1]\n",
      "[[-0.46682878  0.46682878]\n",
      " [ 0.54034963 -0.54034963]\n",
      " [ 0.52750566 -0.52750566]\n",
      " [-0.5870063   0.5870063 ]\n",
      " [ 0.53836556 -0.53836556]]\n",
      "Loss after iteration 1: 0.694472\n",
      "[[ 0.52692121  0.47307879]\n",
      " [ 0.52770369  0.47229631]\n",
      " [ 0.5215541   0.4784459 ]\n",
      " [ 0.42481264  0.57518736]\n",
      " [ 0.52984545  0.47015455]] [0 1 1 0 1]\n",
      "[[-0.47307879  0.47307879]\n",
      " [ 0.52770369 -0.52770369]\n",
      " [ 0.5215541  -0.5215541 ]\n",
      " [-0.57518736  0.57518736]\n",
      " [ 0.52984545 -0.52984545]]\n",
      "Loss after iteration 2: 0.686350\n",
      "[[ 0.52211796  0.47788204]\n",
      " [ 0.50840124  0.49159876]\n",
      " [ 0.51563467  0.48436533]\n",
      " [ 0.42382959  0.57617041]\n",
      " [ 0.52088349  0.47911651]] [0 1 1 0 1]\n",
      "[[-0.47788204  0.47788204]\n",
      " [ 0.50840124 -0.50840124]\n",
      " [ 0.51563467 -0.51563467]\n",
      " [-0.57617041  0.57617041]\n",
      " [ 0.52088349 -0.52088349]]\n",
      "Loss after iteration 3: 0.660560\n",
      "[[ 0.51538237  0.48461763]\n",
      " [ 0.43303518  0.56696482]\n",
      " [ 0.50613357  0.49386643]\n",
      " [ 0.40904374  0.59095626]\n",
      " [ 0.4894682   0.5105318 ]] [0 1 1 0 1]\n",
      "[[-0.48461763  0.48461763]\n",
      " [ 0.43303518 -0.43303518]\n",
      " [ 0.50613357 -0.50613357]\n",
      " [-0.59095626  0.59095626]\n",
      " [ 0.4894682  -0.4894682 ]]\n",
      "Loss after iteration 4: 0.558802\n",
      "[[ 0.46932823  0.53067177]\n",
      " [ 0.19626712  0.80373288]\n",
      " [ 0.46867223  0.53132777]\n",
      " [ 0.3639253   0.6360747 ]\n",
      " [ 0.23757696  0.76242304]] [0 1 1 0 1]\n",
      "[[-0.53067177  0.53067177]\n",
      " [ 0.19626712 -0.19626712]\n",
      " [ 0.46867223 -0.46867223]\n",
      " [-0.6360747   0.6360747 ]\n",
      " [ 0.23757696 -0.23757696]]\n",
      "Loss after iteration 5: 0.548822\n",
      "[[ 0.34359653  0.65640347]\n",
      " [ 0.15399862  0.84600138]\n",
      " [ 0.42321293  0.57678707]\n",
      " [ 0.35592414  0.64407586]\n",
      " [ 0.15759747  0.84240253]] [0 1 1 0 1]\n",
      "[[-0.65640347  0.65640347]\n",
      " [ 0.15399862 -0.15399862]\n",
      " [ 0.42321293 -0.42321293]\n",
      " [-0.64407586  0.64407586]\n",
      " [ 0.15759747 -0.15759747]]\n",
      "Loss after iteration 6: 0.530854\n",
      "[[ 0.50626916  0.49373084]\n",
      " [ 0.18116855  0.81883145]\n",
      " [ 0.51449615  0.48550385]\n",
      " [ 0.43183624  0.56816376]\n",
      " [ 0.20747533  0.79252467]] [0 1 1 0 1]\n",
      "[[-0.49373084  0.49373084]\n",
      " [ 0.18116855 -0.18116855]\n",
      " [ 0.51449615 -0.51449615]\n",
      " [-0.56816376  0.56816376]\n",
      " [ 0.20747533 -0.20747533]]\n",
      "Loss after iteration 7: 0.512533\n",
      "[[ 0.4160159   0.5839841 ]\n",
      " [ 0.15818775  0.84181225]\n",
      " [ 0.49129816  0.50870184]\n",
      " [ 0.40858515  0.59141485]\n",
      " [ 0.16140667  0.83859333]] [0 1 1 0 1]\n",
      "[[-0.5839841   0.5839841 ]\n",
      " [ 0.15818775 -0.15818775]\n",
      " [ 0.49129816 -0.49129816]\n",
      " [-0.59141485  0.59141485]\n",
      " [ 0.16140667 -0.16140667]]\n",
      "Loss after iteration 8: 0.479911\n",
      "[[ 0.55691061  0.44308939]\n",
      " [ 0.17976005  0.82023995]\n",
      " [ 0.57413788  0.42586212]\n",
      " [ 0.45965775  0.54034225]\n",
      " [ 0.19407688  0.80592312]] [0 1 1 0 1]\n",
      "[[-0.44308939  0.44308939]\n",
      " [ 0.17976005 -0.17976005]\n",
      " [ 0.57413788 -0.57413788]\n",
      " [-0.54034225  0.54034225]\n",
      " [ 0.19407688 -0.19407688]]\n",
      "Loss after iteration 9: 0.436473\n",
      "[[ 0.54439863  0.45560137]\n",
      " [ 0.17666485  0.82333515]\n",
      " [ 0.61357743  0.38642257]\n",
      " [ 0.4787525   0.5212475 ]\n",
      " [ 0.18165907  0.81834093]] [0 1 1 0 1]\n",
      "[[-0.45560137  0.45560137]\n",
      " [ 0.17666485 -0.17666485]\n",
      " [ 0.61357743 -0.61357743]\n",
      " [-0.5212475   0.5212475 ]\n",
      " [ 0.18165907 -0.18165907]]\n",
      "Loss after iteration 10: 0.409612\n",
      "[[ 0.67372266  0.32627734]\n",
      " [ 0.19309325  0.80690675]\n",
      " [ 0.71090066  0.28909934]\n",
      " [ 0.51814636  0.48185364]\n",
      " [ 0.2039966   0.7960034 ]] [0 1 1 0 1]\n",
      "[[-0.32627734  0.32627734]\n",
      " [ 0.19309325 -0.19309325]\n",
      " [ 0.71090066 -0.71090066]\n",
      " [-0.48185364  0.48185364]\n",
      " [ 0.2039966  -0.2039966 ]]\n",
      "Loss after iteration 11: 0.397148\n",
      "[[ 0.52609973  0.47390027]\n",
      " [ 0.16170724  0.83829276]\n",
      " [ 0.64240986  0.35759014]\n",
      " [ 0.41178525  0.58821475]\n",
      " [ 0.16530357  0.83469643]] [0 1 1 0 1]\n",
      "[[-0.47390027  0.47390027]\n",
      " [ 0.16170724 -0.16170724]\n",
      " [ 0.64240986 -0.64240986]\n",
      " [-0.58821475  0.58821475]\n",
      " [ 0.16530357 -0.16530357]]\n",
      "Loss after iteration 12: 0.387315\n",
      "[[ 0.70856234  0.29143766]\n",
      " [ 0.1735817   0.8264183 ]\n",
      " [ 0.73562763  0.26437237]\n",
      " [ 0.41858041  0.58141959]\n",
      " [ 0.18452452  0.81547548]] [0 1 1 0 1]\n",
      "[[-0.29143766  0.29143766]\n",
      " [ 0.1735817  -0.1735817 ]\n",
      " [ 0.73562763 -0.73562763]\n",
      " [-0.58141959  0.58141959]\n",
      " [ 0.18452452 -0.18452452]]\n",
      "Loss after iteration 13: 0.383726\n",
      "[[ 0.50872779  0.49127221]\n",
      " [ 0.14908592  0.85091408]\n",
      " [ 0.62476152  0.37523848]\n",
      " [ 0.31337165  0.68662835]\n",
      " [ 0.15223039  0.84776961]] [0 1 1 0 1]\n",
      "[[-0.49127221  0.49127221]\n",
      " [ 0.14908592 -0.14908592]\n",
      " [ 0.62476152 -0.62476152]\n",
      " [-0.68662835  0.68662835]\n",
      " [ 0.15223039 -0.15223039]]\n",
      "Loss after iteration 14: 0.377091\n",
      "[[ 0.72695248  0.27304752]\n",
      " [ 0.16016173  0.83983827]\n",
      " [ 0.73833985  0.26166015]\n",
      " [ 0.33095599  0.66904401]\n",
      " [ 0.17072429  0.82927571]] [0 1 1 0 1]\n",
      "[[-0.27304752  0.27304752]\n",
      " [ 0.16016173 -0.16016173]\n",
      " [ 0.73833985 -0.73833985]\n",
      " [-0.66904401  0.66904401]\n",
      " [ 0.17072429 -0.17072429]]\n",
      "Loss after iteration 15: 0.376302\n",
      "[[ 0.50750655  0.49249345]\n",
      " [ 0.14283724  0.85716276]\n",
      " [ 0.61181032  0.38818968]\n",
      " [ 0.25940228  0.74059772]\n",
      " [ 0.14571377  0.85428623]] [0 1 1 0 1]\n",
      "[[-0.49249345  0.49249345]\n",
      " [ 0.14283724 -0.14283724]\n",
      " [ 0.61181032 -0.61181032]\n",
      " [-0.74059772  0.74059772]\n",
      " [ 0.14571377 -0.14571377]]\n",
      "Loss after iteration 16: 0.370870\n",
      "[[ 0.73532379  0.26467621]\n",
      " [ 0.15191261  0.84808739]\n",
      " [ 0.73511567  0.26488433]\n",
      " [ 0.27729056  0.72270944]\n",
      " [ 0.1614932   0.8385068 ]] [0 1 1 0 1]\n",
      "[[-0.26467621  0.26467621]\n",
      " [ 0.15191261 -0.15191261]\n",
      " [ 0.73511567 -0.73511567]\n",
      " [-0.72270944  0.72270944]\n",
      " [ 0.1614932  -0.1614932 ]]\n",
      "Loss after iteration 17: 0.370774\n",
      "[[ 0.52188354  0.47811646]\n",
      " [ 0.1394238   0.8605762 ]\n",
      " [ 0.60762551  0.39237449]\n",
      " [ 0.22921506  0.77078494]\n",
      " [ 0.14223489  0.85776511]] [0 1 1 0 1]\n",
      "[[-0.47811646  0.47811646]\n",
      " [ 0.1394238  -0.1394238 ]\n",
      " [ 0.60762551 -0.60762551]\n",
      " [-0.77078494  0.77078494]\n",
      " [ 0.14223489 -0.14223489]]\n",
      "Loss after iteration 18: 0.366686\n",
      "[[ 0.73864203  0.26135797]\n",
      " [ 0.14649511  0.85350489]\n",
      " [ 0.72911197  0.27088803]\n",
      " [ 0.24370696  0.75629304]\n",
      " [ 0.15508077  0.84491923]] [0 1 1 0 1]\n",
      "[[-0.26135797  0.26135797]\n",
      " [ 0.14649511 -0.14649511]\n",
      " [ 0.72911197 -0.72911197]\n",
      " [-0.75629304  0.75629304]\n",
      " [ 0.15508077 -0.15508077]]\n",
      "Loss after iteration 19: 0.366709\n",
      "[[ 0.54079725  0.45920275]\n",
      " [ 0.13723078  0.86276922]\n",
      " [ 0.60763176  0.39236824]\n",
      " [ 0.21087113  0.78912887]\n",
      " [ 0.14004315  0.85995685]] [0 1 1 0 1]\n",
      "[[-0.45920275  0.45920275]\n",
      " [ 0.13723078 -0.13723078]\n",
      " [ 0.60763176 -0.60763176]\n",
      " [-0.78912887  0.78912887]\n",
      " [ 0.14004315 -0.14004315]]\n",
      "Loss after iteration 20: 0.363789\n",
      "[[ 0.73970906  0.26029094]\n",
      " [ 0.14278041  0.85721959]\n",
      " [ 0.722882    0.277118  ]\n",
      " [ 0.222568    0.777432  ]\n",
      " [ 0.15051112  0.84948888]] [0 1 1 0 1]\n",
      "[[-0.26029094  0.26029094]\n",
      " [ 0.14278041 -0.14278041]\n",
      " [ 0.722882   -0.722882  ]\n",
      " [-0.777432    0.777432  ]\n",
      " [ 0.15051112 -0.15051112]]\n",
      "Loss after iteration 21: 0.363783\n",
      "[[ 0.55916387  0.44083613]\n",
      " [ 0.13566982  0.86433018]\n",
      " [ 0.60991844  0.39008156]\n",
      " [ 0.19924558  0.80075442]\n",
      " [ 0.13849263  0.86150737]] [0 1 1 0 1]\n",
      "[[-0.44083613  0.44083613]\n",
      " [ 0.13566982 -0.13566982]\n",
      " [ 0.60991844 -0.60991844]\n",
      " [-0.80075442  0.80075442]\n",
      " [ 0.13849263 -0.13849263]]\n",
      "Loss after iteration 22: 0.361696\n",
      "[[ 0.73944933  0.26055067]\n",
      " [ 0.14010672  0.85989328]\n",
      " [ 0.71729525  0.28270475]\n",
      " [ 0.20884745  0.79115255]\n",
      " [ 0.14711882  0.85288118]] [0 1 1 0 1]\n",
      "[[-0.26055067  0.26055067]\n",
      " [ 0.14010672 -0.14010672]\n",
      " [ 0.71729525 -0.71729525]\n",
      " [-0.79115255  0.79115255]\n",
      " [ 0.14711882 -0.14711882]]\n",
      "Loss after iteration 23: 0.361632\n",
      "[[ 0.5760273   0.4239727 ]\n",
      " [ 0.13449893  0.86550107]\n",
      " [ 0.61371626  0.38628374]\n",
      " [ 0.19155733  0.80844267]\n",
      " [ 0.1373352   0.8626648 ]] [0 1 1 0 1]\n",
      "[[-0.4239727   0.4239727 ]\n",
      " [ 0.13449893 -0.13449893]\n",
      " [ 0.61371626 -0.61371626]\n",
      " [-0.80844267  0.80844267]\n",
      " [ 0.1373352  -0.1373352 ]]\n",
      "Loss after iteration 24: 0.360125\n",
      "[[ 0.73829417  0.26170583]\n",
      " [ 0.13809933  0.86190067]\n",
      " [ 0.71236771  0.28763229]\n",
      " [ 0.19947989  0.80052011]\n",
      " [ 0.14451577  0.85548423]] [0 1 1 0 1]\n",
      "[[-0.26170583  0.26170583]\n",
      " [ 0.13809933 -0.13809933]\n",
      " [ 0.71236771 -0.71236771]\n",
      " [-0.80052011  0.80052011]\n",
      " [ 0.14451577 -0.14451577]]\n",
      "Loss after iteration 25: 0.360018\n",
      "[[ 0.59105772  0.40894228]\n",
      " [ 0.13358539  0.86641461]\n",
      " [ 0.61818585  0.38181415]\n",
      " [ 0.18620884  0.81379116]\n",
      " [ 0.13643824  0.86356176]] [0 1 1 0 1]\n",
      "[[-0.40894228  0.40894228]\n",
      " [ 0.13358539 -0.13358539]\n",
      " [ 0.61818585 -0.61818585]\n",
      " [-0.81379116  0.81379116]\n",
      " [ 0.13643824 -0.13643824]]\n",
      "Loss after iteration 26: 0.358919\n",
      "[[ 0.73660009  0.26339991]\n",
      " [ 0.13654567  0.86345433]\n",
      " [ 0.70802096  0.29197904]\n",
      " [ 0.19279024  0.80720976]\n",
      " [ 0.14247559  0.85752441]] [0 1 1 0 1]\n",
      "[[-0.26339991  0.26339991]\n",
      " [ 0.13654567 -0.13654567]\n",
      " [ 0.70802096 -0.70802096]\n",
      " [-0.80720976  0.80720976]\n",
      " [ 0.14247559 -0.14247559]]\n",
      "Loss after iteration 27: 0.358792\n",
      "[[ 0.60407236  0.39592764]\n",
      " [ 0.13284919  0.86715081]\n",
      " [ 0.622691    0.377309  ]\n",
      " [ 0.18232169  0.81767831]\n",
      " [ 0.13572006  0.86427994]] [0 1 1 0 1]\n",
      "[[-0.39592764  0.39592764]\n",
      " [ 0.13284919 -0.13284919]\n",
      " [ 0.622691   -0.622691  ]\n",
      " [-0.81767831  0.81767831]\n",
      " [ 0.13572006 -0.13572006]]\n",
      "Loss after iteration 28: 0.357983\n",
      "[[ 0.73466084  0.26533916]\n",
      " [ 0.13531649  0.86468351]\n",
      " [ 0.70421845  0.29578155]\n",
      " [ 0.18785129  0.81214871]\n",
      " [ 0.1408536   0.8591464 ]] [0 1 1 0 1]\n",
      "[[-0.26533916  0.26533916]\n",
      " [ 0.13531649 -0.13531649]\n",
      " [ 0.70421845 -0.70421845]\n",
      " [-0.81214871  0.81214871]\n",
      " [ 0.1408536  -0.1408536 ]]\n",
      "Loss after iteration 29: 0.357854\n",
      "[[ 0.61506508  0.38493492]\n",
      " [ 0.13224033  0.86775967]\n",
      " [ 0.62686435  0.37313565]\n",
      " [ 0.17940073  0.82059927]\n",
      " [ 0.13512874  0.86487126]] [0 1 1 0 1]\n",
      "[[-0.38493492  0.38493492]\n",
      " [ 0.13224033 -0.13224033]\n",
      " [ 0.62686435 -0.62686435]\n",
      " [-0.82059927  0.82059927]\n",
      " [ 0.13512874 -0.13512874]]\n",
      "Loss after iteration 30: 0.357249\n",
      "[[ 0.73269821  0.26730179]\n",
      " [ 0.13432748  0.86567252]\n",
      " [ 0.70094048  0.29905952]\n",
      " [ 0.18411453  0.81588547]\n",
      " [ 0.13955031  0.86044969]] [0 1 1 0 1]\n",
      "[[-0.26730179  0.26730179]\n",
      " [ 0.13432748 -0.13432748]\n",
      " [ 0.70094048 -0.70094048]\n",
      " [-0.81588547  0.81588547]\n",
      " [ 0.13955031 -0.13955031]]\n",
      "Loss after iteration 31: 0.357129\n",
      "[[ 0.62415224  0.37584776]\n",
      " [ 0.13172619  0.86827381]\n",
      " [ 0.63052306  0.36947694]\n",
      " [ 0.17714918  0.82285082]\n",
      " [ 0.13463018  0.86536982]] [0 1 1 0 1]\n",
      "[[-0.37584776  0.37584776]\n",
      " [ 0.13172619 -0.13172619]\n",
      " [ 0.63052306 -0.63052306]\n",
      " [-0.82285082  0.82285082]\n",
      " [ 0.13463018 -0.13463018]]\n",
      "Loss after iteration 32: 0.356670\n",
      "[[ 0.73087034  0.26912966]\n",
      " [ 0.13352084  0.86647916]\n",
      " [ 0.69816776  0.30183224]\n",
      " [ 0.18123509  0.81876491]\n",
      " [ 0.13849445  0.86150555]] [0 1 1 0 1]\n",
      "[[-0.26912966  0.26912966]\n",
      " [ 0.13352084 -0.13352084]\n",
      " [ 0.69816776 -0.69816776]\n",
      " [-0.81876491  0.81876491]\n",
      " [ 0.13849445 -0.13849445]]\n",
      "Loss after iteration 33: 0.356564\n",
      "[[ 0.63150618  0.36849382]\n",
      " [ 0.13128446  0.86871554]\n",
      " [ 0.63359247  0.36640753]\n",
      " [ 0.17537883  0.82462117]\n",
      " [ 0.13420106  0.86579894]] [0 1 1 0 1]\n",
      "[[-0.36849382  0.36849382]\n",
      " [ 0.13128446 -0.13128446]\n",
      " [ 0.63359247 -0.63359247]\n",
      " [-0.82462117  0.82462117]\n",
      " [ 0.13420106 -0.13420106]]\n",
      "Loss after iteration 34: 0.356209\n",
      "[[ 0.72928501  0.27071499]\n",
      " [ 0.13285552  0.86714448]\n",
      " [ 0.69587823  0.30412177]\n",
      " [ 0.17898605  0.82101395]\n",
      " [ 0.13763366  0.86236634]] [0 1 1 0 1]\n",
      "[[-0.27071499  0.27071499]\n",
      " [ 0.13285552 -0.13285552]\n",
      " [ 0.69587823 -0.69587823]\n",
      " [-0.82101395  0.82101395]\n",
      " [ 0.13763366 -0.13763366]]\n",
      "Loss after iteration 35: 0.356120\n",
      "[[ 0.63731094  0.36268906]\n",
      " [ 0.13089933  0.86910067]\n",
      " [ 0.63605732  0.36394268]\n",
      " [ 0.17396479  0.82603521]\n",
      " [ 0.13382488  0.86617512]] [0 1 1 0 1]\n",
      "[[-0.36268906  0.36268906]\n",
      " [ 0.13089933 -0.13089933]\n",
      " [ 0.63605732 -0.63605732]\n",
      " [-0.82603521  0.82603521]\n",
      " [ 0.13382488 -0.13382488]]\n",
      "Loss after iteration 36: 0.355840\n",
      "[[ 0.72801295  0.27198705]\n",
      " [ 0.13230167  0.86769833]\n",
      " [ 0.69404841  0.30595159]\n",
      " [ 0.1772127   0.8227873 ]\n",
      " [ 0.13692889  0.86307111]] [0 1 1 0 1]\n",
      "[[-0.27198705  0.27198705]\n",
      " [ 0.13230167 -0.13230167]\n",
      " [ 0.69404841 -0.69404841]\n",
      " [-0.8227873   0.8227873 ]\n",
      " [ 0.13692889 -0.13692889]]\n",
      "Loss after iteration 37: 0.355769\n",
      "[[ 0.6417369   0.3582631 ]\n",
      " [ 0.13055915  0.86944085]\n",
      " [ 0.63793146  0.36206854]\n",
      " [ 0.17282102  0.82717898]\n",
      " [ 0.13348949  0.86651051]] [0 1 1 0 1]\n",
      "[[-0.3582631   0.3582631 ]\n",
      " [ 0.13055915 -0.13055915]\n",
      " [ 0.63793146 -0.63793146]\n",
      " [-0.82717898  0.82717898]\n",
      " [ 0.13348949 -0.13348949]]\n",
      "Loss after iteration 38: 0.355543\n",
      "[[ 0.72710018  0.27289982]\n",
      " [ 0.13183718  0.86816282]\n",
      " [ 0.69265606  0.30734394]\n",
      " [ 0.17580648  0.82419352]\n",
      " [ 0.13635072  0.86364928]] [0 1 1 0 1]\n",
      "[[-0.27289982  0.27289982]\n",
      " [ 0.13183718 -0.13183718]\n",
      " [ 0.69265606 -0.69265606]\n",
      " [-0.82419352  0.82419352]\n",
      " [ 0.13635072 -0.13635072]]\n",
      "Loss after iteration 39: 0.355490\n",
      "[[ 0.64492726  0.35507274]\n",
      " [ 0.13025506  0.86974494]\n",
      " [ 0.63923902  0.36076098]\n",
      " [ 0.17188614  0.82811386]\n",
      " [ 0.13318559  0.86681441]] [0 1 1 0 1]\n",
      "[[-0.35507274  0.35507274]\n",
      " [ 0.13025506 -0.13025506]\n",
      " [ 0.63923902 -0.63923902]\n",
      " [-0.82811386  0.82811386]\n",
      " [ 0.13318559 -0.13318559]]\n",
      "Loss after iteration 40: 0.355305\n",
      "[[ 0.7265786   0.2734214 ]\n",
      " [ 0.13144546  0.86855454]\n",
      " [ 0.69168309  0.30831691]\n",
      " [ 0.17468925  0.82531075]\n",
      " [ 0.13587695  0.86412305]] [0 1 1 0 1]\n",
      "[[-0.2734214   0.2734214 ]\n",
      " [ 0.13144546 -0.13144546]\n",
      " [ 0.69168309 -0.69168309]\n",
      " [-0.82531075  0.82531075]\n",
      " [ 0.13587695 -0.13587695]]\n",
      "Loss after iteration 41: 0.355269\n",
      "[[ 0.64699115  0.35300885]\n",
      " [ 0.1299801   0.8700199 ]\n",
      " [ 0.64000256  0.35999744]\n",
      " [ 0.17111478  0.82888522]\n",
      " [ 0.13290572  0.86709428]] [0 1 1 0 1]\n",
      "[[-0.35300885  0.35300885]\n",
      " [ 0.1299801  -0.1299801 ]\n",
      " [ 0.64000256 -0.64000256]\n",
      " [-0.82888522  0.82888522]\n",
      " [ 0.13290572 -0.13290572]]\n",
      "Loss after iteration 42: 0.355115\n",
      "[[ 0.72647434  0.27352566]\n",
      " [ 0.131114    0.868886  ]\n",
      " [ 0.69111799  0.30888201]\n",
      " [ 0.1738036   0.8261964 ]\n",
      " [ 0.13549085  0.86450915]] [0 1 1 0 1]\n",
      "[[-0.27352566  0.27352566]\n",
      " [ 0.131114   -0.131114  ]\n",
      " [ 0.69111799 -0.69111799]\n",
      " [-0.8261964   0.8261964 ]\n",
      " [ 0.13549085 -0.13549085]]\n",
      "Loss after iteration 43: 0.355097\n",
      "[[ 0.64800061  0.35199939]\n",
      " [ 0.12972862  0.87027138]\n",
      " [ 0.6402359   0.3597641 ]\n",
      " [ 0.1704722   0.8295278 ]\n",
      " [ 0.13264362  0.86735638]] [0 1 1 0 1]\n",
      "[[-0.35199939  0.35199939]\n",
      " [ 0.12972862 -0.12972862]\n",
      " [ 0.6402359  -0.6402359 ]\n",
      " [-0.8295278   0.8295278 ]\n",
      " [ 0.13264362 -0.13264362]]\n",
      "Loss after iteration 44: 0.354967\n",
      "[[ 0.72681375  0.27318625]\n",
      " [ 0.13083333  0.86916667]\n",
      " [ 0.69095762  0.30904238]\n",
      " [ 0.17310656  0.82689344]\n",
      " [ 0.13517996  0.86482004]] [0 1 1 0 1]\n",
      "[[-0.27318625  0.27318625]\n",
      " [ 0.13083333 -0.13083333]\n",
      " [ 0.69095762 -0.69095762]\n",
      " [-0.82689344  0.82689344]\n",
      " [ 0.13517996 -0.13517996]]\n",
      "Loss after iteration 45: 0.354969\n",
      "[[ 0.64798961  0.35201039]\n",
      " [ 0.1294959   0.8705041 ]\n",
      " [ 0.63993993  0.36006007]\n",
      " [ 0.16993078  0.83006922]\n",
      " [ 0.13239376  0.86760624]] [0 1 1 0 1]\n",
      "[[-0.35201039  0.35201039]\n",
      " [ 0.1294959  -0.1294959 ]\n",
      " [ 0.63993993 -0.63993993]\n",
      " [-0.83006922  0.83006922]\n",
      " [ 0.13239376 -0.13239376]]\n",
      "Loss after iteration 46: 0.354859\n",
      "[[ 0.7276266   0.2723734 ]\n",
      " [ 0.13059634  0.86940366]\n",
      " [ 0.69120787  0.30879213]\n",
      " [ 0.17256539  0.82743461]\n",
      " [ 0.13493518  0.86506482]] [0 1 1 0 1]\n",
      "[[-0.2723734   0.2723734 ]\n",
      " [ 0.13059634 -0.13059634]\n",
      " [ 0.69120787 -0.69120787]\n",
      " [-0.82743461  0.82743461]\n",
      " [ 0.13493518 -0.13493518]]\n",
      "Loss after iteration 47: 0.354884\n",
      "[[ 0.64695463  0.35304537]\n",
      " [ 0.12927788  0.87072212]\n",
      " [ 0.63910069  0.36089931]\n",
      " [ 0.16946767  0.83053233]\n",
      " [ 0.1321511   0.8678489 ]] [0 1 1 0 1]\n",
      "[[-0.35304537  0.35304537]\n",
      " [ 0.12927788 -0.12927788]\n",
      " [ 0.63910069 -0.63910069]\n",
      " [-0.83053233  0.83053233]\n",
      " [ 0.1321511  -0.1321511 ]]\n",
      "Loss after iteration 48: 0.354789\n",
      "[[ 0.72894624  0.27105376]\n",
      " [ 0.13039768  0.86960232]\n",
      " [ 0.69188296  0.30811704]\n",
      " [ 0.17215472  0.82784528]\n",
      " [ 0.13475003  0.86524997]] [0 1 1 0 1]\n",
      "[[-0.27105376  0.27105376]\n",
      " [ 0.13039768 -0.13039768]\n",
      " [ 0.69188296 -0.69188296]\n",
      " [-0.82784528  0.82784528]\n",
      " [ 0.13475003 -0.13475003]]\n",
      "Loss after iteration 49: 0.354842\n",
      "[[ 0.64485762  0.35514238]\n",
      " [ 0.12907107  0.87092893]\n",
      " [ 0.63768989  0.36231011]\n",
      " [ 0.16906328  0.83093672]\n",
      " [ 0.13191101  0.86808899]] [0 1 1 0 1]\n",
      "[[-0.35514238  0.35514238]\n",
      " [ 0.12907107 -0.12907107]\n",
      " [ 0.63768989 -0.63768989]\n",
      " [-0.83093672  0.83093672]\n",
      " [ 0.13191101 -0.13191101]]\n",
      "Loss after iteration 50: 0.354760\n",
      "[[ 0.73080569  0.26919431]\n",
      " [ 0.13023343  0.86976657]\n",
      " [ 0.69300242  0.30699758]\n",
      " [ 0.17185446  0.82814554]\n",
      " [ 0.13462005  0.86537995]] [0 1 1 0 1]\n",
      "[[-0.26919431  0.26919431]\n",
      " [ 0.13023343 -0.13023343]\n",
      " [ 0.69300242 -0.69300242]\n",
      " [-0.82814554  0.82814554]\n",
      " [ 0.13462005 -0.13462005]]\n",
      "Loss after iteration 51: 0.354848\n",
      "[[ 0.64163348  0.35836652]\n",
      " [ 0.12887249  0.87112751]\n",
      " [ 0.63566876  0.36433124]\n",
      " [ 0.16870028  0.83129972]\n",
      " [ 0.13166934  0.86833066]] [0 1 1 0 1]\n",
      "[[-0.35836652  0.35836652]\n",
      " [ 0.12887249 -0.12887249]\n",
      " [ 0.63566876 -0.63566876]\n",
      " [-0.83129972  0.83129972]\n",
      " [ 0.13166934 -0.13166934]]\n",
      "Loss after iteration 52: 0.354777\n",
      "[[ 0.73322846  0.26677154]\n",
      " [ 0.13010058  0.86989942]\n",
      " [ 0.69458475  0.30541525]\n",
      " [ 0.17164814  0.82835186]\n",
      " [ 0.13454201  0.86545799]] [0 1 1 0 1]\n",
      "[[-0.26677154  0.26677154]\n",
      " [ 0.13010058 -0.13010058]\n",
      " [ 0.69458475 -0.69458475]\n",
      " [-0.82835186  0.82835186]\n",
      " [ 0.13454201 -0.13454201]]\n",
      "Loss after iteration 53: 0.354911\n",
      "[[ 0.63720675  0.36279325]\n",
      " [ 0.12867979  0.87132021]\n",
      " [ 0.63299753  0.36700247]\n",
      " [ 0.16836321  0.83163679]\n",
      " [ 0.13142278  0.86857722]] [0 1 1 0 1]\n",
      "[[-0.36279325  0.36279325]\n",
      " [ 0.12867979 -0.12867979]\n",
      " [ 0.63299753 -0.63299753]\n",
      " [-0.83163679  0.83163679]\n",
      " [ 0.13142278 -0.13142278]]\n",
      "Loss after iteration 54: 0.354844\n",
      "[[ 0.7362126   0.2637874 ]\n",
      " [ 0.12999666  0.87000334]\n",
      " [ 0.6966361   0.3033639 ]\n",
      " [ 0.17152147  0.82847853]\n",
      " [ 0.13451295  0.86548705]] [0 1 1 0 1]\n",
      "[[-0.2637874   0.2637874 ]\n",
      " [ 0.12999666 -0.12999666]\n",
      " [ 0.6966361  -0.6966361 ]\n",
      " [-0.82847853  0.82847853]\n",
      " [ 0.13451295 -0.13451295]]\n",
      "Loss after iteration 55: 0.355037\n",
      "[[ 0.63152423  0.36847577]\n",
      " [ 0.12849151  0.87150849]\n",
      " [ 0.62965383  0.37034617]\n",
      " [ 0.1680386   0.8319614 ]\n",
      " [ 0.1311695   0.8688305 ]] [0 1 1 0 1]\n",
      "[[-0.36847577  0.36847577]\n",
      " [ 0.12849151 -0.12849151]\n",
      " [ 0.62965383 -0.62965383]\n",
      " [-0.8319614   0.8319614 ]\n",
      " [ 0.1311695  -0.1311695 ]]\n",
      "Loss after iteration 56: 0.354968\n",
      "[[ 0.73970793  0.26029207]\n",
      " [ 0.12991916  0.87008084]\n",
      " [ 0.69913302  0.30086698]\n",
      " [ 0.17146086  0.82853914]\n",
      " [ 0.13452882  0.86547118]] [0 1 1 0 1]\n",
      "[[-0.26029207  0.26029207]\n",
      " [ 0.12991916 -0.12991916]\n",
      " [ 0.69913302 -0.69913302]\n",
      " [-0.82853914  0.82853914]\n",
      " [ 0.13452882 -0.13452882]]\n",
      "Loss after iteration 57: 0.355235\n",
      "[[ 0.6246107   0.3753893 ]\n",
      " [ 0.12830752  0.87169248]\n",
      " [ 0.62566306  0.37433694]\n",
      " [ 0.1677158   0.8322842 ]\n",
      " [ 0.13091011  0.86908989]] [0 1 1 0 1]\n",
      "[[-0.3753893   0.3753893 ]\n",
      " [ 0.12830752 -0.12830752]\n",
      " [ 0.62566306 -0.62566306]\n",
      " [-0.8322842   0.8322842 ]\n",
      " [ 0.13091011 -0.13091011]]\n",
      "Loss after iteration 58: 0.355149\n",
      "[[ 0.74359073  0.25640927]\n",
      " [ 0.12986479  0.87013521]\n",
      " [ 0.70200051  0.29799949]\n",
      " [ 0.17145177  0.82854823]\n",
      " [ 0.13458272  0.86541728]] [0 1 1 0 1]\n",
      "[[-0.25640927  0.25640927]\n",
      " [ 0.12986479 -0.12986479]\n",
      " [ 0.70200051 -0.70200051]\n",
      " [-0.82854823  0.82854823]\n",
      " [ 0.13458272 -0.13458272]]\n",
      "Loss after iteration 59: 0.355507\n",
      "[[ 0.61664766  0.38335234]\n",
      " [ 0.1281295   0.8718705 ]\n",
      " [ 0.62113923  0.37886077]\n",
      " [ 0.16738857  0.83261143]\n",
      " [ 0.13064877  0.86935123]] [0 1 1 0 1]\n",
      "[[-0.38335234  0.38335234]\n",
      " [ 0.1281295  -0.1281295 ]\n",
      " [ 0.62113923 -0.62113923]\n",
      " [-0.83261143  0.83261143]\n",
      " [ 0.13064877 -0.13064877]]\n",
      "Loss after iteration 60: 0.355380\n",
      "[[ 0.74764775  0.25235225]\n",
      " [ 0.12982894  0.87017106]\n",
      " [ 0.70509231  0.29490769]\n",
      " [ 0.17147702  0.82852298]\n",
      " [ 0.13466316  0.86533684]] [0 1 1 0 1]\n",
      "[[-0.25235225  0.25235225]\n",
      " [ 0.12982894 -0.12982894]\n",
      " [ 0.70509231 -0.70509231]\n",
      " [-0.82852298  0.82852298]\n",
      " [ 0.13466316 -0.13466316]]\n",
      "Loss after iteration 61: 0.355839\n",
      "[[ 0.60805227  0.39194773]\n",
      " [ 0.12796128  0.87203872]\n",
      " [ 0.61632266  0.38367734]\n",
      " [ 0.16705675  0.83294325]\n",
      " [ 0.13039386  0.86960614]] [0 1 1 0 1]\n",
      "[[-0.39194773  0.39194773]\n",
      " [ 0.12796128 -0.12796128]\n",
      " [ 0.61632266 -0.61632266]\n",
      " [-0.83294325  0.83294325]\n",
      " [ 0.13039386 -0.13039386]]\n",
      "Loss after iteration 62: 0.355639\n",
      "[[ 0.75158813  0.24841187]\n",
      " [ 0.12980539  0.87019461]\n",
      " [ 0.70818775  0.29181225]\n",
      " [ 0.17151535  0.82848465]\n",
      " [ 0.13475368  0.86524632]] [0 1 1 0 1]\n",
      "[[-0.24841187  0.24841187]\n",
      " [ 0.12980539 -0.12980539]\n",
      " [ 0.70818775 -0.70818775]\n",
      " [-0.82848465  0.82848465]\n",
      " [ 0.13475368 -0.13475368]]\n",
      "Loss after iteration 63: 0.356202\n",
      "[[ 0.59949767  0.40050233]\n",
      " [ 0.12780844  0.87219156]\n",
      " [ 0.61158322  0.38841678]\n",
      " [ 0.166727    0.833273  ]\n",
      " [ 0.1301573   0.8698427 ]] [0 1 1 0 1]\n",
      "[[-0.40050233  0.40050233]\n",
      " [ 0.12780844 -0.12780844]\n",
      " [ 0.61158322 -0.61158322]\n",
      " [-0.833273    0.833273  ]\n",
      " [ 0.1301573  -0.1301573 ]]\n",
      "Loss after iteration 64: 0.355894\n",
      "[[ 0.75509755  0.24490245]\n",
      " [ 0.12978709  0.87021291]\n",
      " [ 0.71102149  0.28897851]\n",
      " [ 0.17154201  0.82845799]\n",
      " [ 0.13483529  0.86516471]] [0 1 1 0 1]\n",
      "[[-0.24490245  0.24490245]\n",
      " [ 0.12978709 -0.12978709]\n",
      " [ 0.71102149 -0.71102149]\n",
      " [-0.82845799  0.82845799]\n",
      " [ 0.13483529 -0.13483529]]\n",
      "Loss after iteration 65: 0.356547\n",
      "[[ 0.59180145  0.40819855]\n",
      " [ 0.12767705  0.87232295]\n",
      " [ 0.60735352  0.39264648]\n",
      " [ 0.1664113   0.8335887 ]\n",
      " [ 0.1299519   0.8700481 ]] [0 1 1 0 1]\n",
      "[[-0.40819855  0.40819855]\n",
      " [ 0.12767705 -0.12767705]\n",
      " [ 0.60735352 -0.60735352]\n",
      " [-0.8335887   0.8335887 ]\n",
      " [ 0.1299519  -0.1299519 ]]\n",
      "Loss after iteration 66: 0.356108\n",
      "[[ 0.75792264  0.24207736]\n",
      " [ 0.12976785  0.87023215]\n",
      " [ 0.71334822  0.28665178]\n",
      " [ 0.17153302  0.82846698]\n",
      " [ 0.13489183  0.86510817]] [0 1 1 0 1]\n",
      "[[-0.24207736  0.24207736]\n",
      " [ 0.12976785 -0.12976785]\n",
      " [ 0.71334822 -0.71334822]\n",
      " [-0.82846698  0.82846698]\n",
      " [ 0.13489183 -0.13489183]]\n",
      "Loss after iteration 67: 0.356828\n",
      "[[ 0.58567773  0.41432227]\n",
      " [ 0.12757158  0.87242842]\n",
      " [ 0.60399573  0.39600427]\n",
      " [ 0.1661231   0.8338769 ]\n",
      " [ 0.12978743  0.87021257]] [0 1 1 0 1]\n",
      "[[-0.41432227  0.41432227]\n",
      " [ 0.12757158 -0.12757158]\n",
      " [ 0.60399573 -0.60399573]\n",
      " [-0.8338769   0.8338769 ]\n",
      " [ 0.12978743 -0.12978743]]\n",
      "Loss after iteration 68: 0.356255\n",
      "[[ 0.75994181  0.24005819]\n",
      " [ 0.12974421  0.87025579]\n",
      " [ 0.71501524  0.28498476]\n",
      " [ 0.17147334  0.82852666]\n",
      " [ 0.13491556  0.86508444]] [0 1 1 0 1]\n",
      "[[-0.24005819  0.24005819]\n",
      " [ 0.12974421 -0.12974421]\n",
      " [ 0.71501524 -0.71501524]\n",
      " [-0.82852666  0.82852666]\n",
      " [ 0.13491556 -0.13491556]]\n",
      "Loss after iteration 69: 0.357012\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "\n",
    "# Generate a dataset and plot it\n",
    "np.random.seed(0)\n",
    "X, y = sklearn.datasets.make_moons(200, noise=0.20)\n",
    "# plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)\n",
    "# plt.show()\n",
    "\n",
    "layers_dim = [2, 3, 2]\n",
    "\n",
    "model = Model(layers_dim)\n",
    "model.train(X, y, num_passes=70, epsilon=0.01, reg_lambda=0.01, print_loss=True)\n",
    "\n",
    "# # Plot the decision boundary\n",
    "# plot_decision_boundary(lambda x: model.predict(x), X, y)\n",
    "# plt.title(\"Decision Boundary for hidden layer size 3\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
